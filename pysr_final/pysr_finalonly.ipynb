{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e649e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, time, os, random\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "# accelerate huggingface to GPU\n",
    "if torch.cuda.is_available():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "from pysr import pysr, best\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "print('Loading data')\n",
    "\n",
    "# xs = pickle.load(open(osp.expanduser(f\"~/../../../scratch/gpfs/cj1223/GraphStorage/standard_raw_final_6t/xs.pkl\"), 'rb'))\n",
    "# ys = pickle.load(open(osp.expanduser(f\"~/../../../scratch/gpfs/cj1223/GraphStorage/standard_raw_final_6t/ys.pkl\"), 'rb'))\n",
    "# n_targ = len(ys[0])\n",
    "# print('Loaded data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f08a17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pysr in module pysr.sr:\n",
      "\n",
      "pysr(X, y, weights=None, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pysr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa16c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ReLU, Linear, Module, LayerNorm, Sequential\n",
    "class MLP(Module):\n",
    "    def __init__(self, n_in=len(xs[0]), n_out = len(ys[0]), hidden=128, nlayers=6, layer_norm=True):\n",
    "        super().__init__()\n",
    "        layers = [Linear(n_in, hidden), ReLU()]\n",
    "        for i in range(nlayers):\n",
    "            layers.append(Linear(hidden, hidden))\n",
    "            layers.append(ReLU()) \n",
    "        if layer_norm:\n",
    "            layers.append(LayerNorm(hidden)) #yay\n",
    "        layers.append(Linear(hidden, n_out))\n",
    "        self.mlp = Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class HaloData(Dataset):\n",
    "    def __init__(self, x, y, xcols = np.arange(34), ycols = np.arange(6)):\n",
    "        self.x = x[:, xcols]\n",
    "        self.y = y[:, ycols]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "030cb3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU  False\n"
     ]
    }
   ],
   "source": [
    "# data=data\n",
    "batch_size=int(2**8) # 2^8 = 256\n",
    "split=0.8\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "l1_lambda = 1e-4\n",
    "l2_lambda = 0\n",
    "hidden = 128\n",
    "\n",
    "train_data = HaloData(xs[:int(len(ys)*split)], ys[:int(len(ys)*split)])\n",
    "test_data = HaloData(xs[int(len(ys)*split):], ys[int(len(ys)*split):])\n",
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "yss, preds=[],[]\n",
    "model = MLP(hidden=hidden)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "print('GPU ', next(model.parameters()).is_cuda)\n",
    "# Initialize our train function\n",
    "def train():\n",
    "    model.train()\n",
    "    for dat in train_loader: \n",
    "        x, y = dat\n",
    "        out = model(x)  \n",
    "        loss = criterion(out, y.view(-1,n_targ)) \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "\n",
    "        loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "#     print(loss, l1_norm*l1_lambda, l2_norm*l2_lambda)\n",
    "\n",
    "\n",
    "##### test function #####\n",
    "\n",
    "def test(loader): \n",
    "    model.eval()\n",
    "    outs = []\n",
    "    ys = []\n",
    "    with torch.no_grad(): \n",
    "        for dat in loader: \n",
    "            x, y = dat\n",
    "            out = model(x) \n",
    "            ys.append(y.view(-1,n_targ))\n",
    "            outs.append(out)\n",
    "    outss=torch.vstack(outs)\n",
    "    yss=torch.vstack(ys)\n",
    "    return torch.std(outss - yss, axis=0), outss, yss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbf75f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███                                                           | 10/200 [00:23<08:05,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train scatter: [0.1638 0.2226 0.273  0.4054 0.4029 0.1949] \n",
      "         Test scatter: [0.156  0.2227 0.2627 0.3987 0.3962 0.1916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████▏                                                       | 20/200 [00:50<09:08,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Train scatter: [0.1637 0.2212 0.2699 0.4017 0.3995 0.1922] \n",
      "         Test scatter: [0.1558 0.2213 0.2594 0.3973 0.3948 0.1889]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████▎                                                    | 30/200 [01:14<07:22,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Train scatter: [0.1574 0.2197 0.2605 0.4013 0.3988 0.1899] \n",
      "         Test scatter: [0.1488 0.2199 0.2488 0.3975 0.3946 0.1866]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████▍                                                 | 40/200 [01:37<06:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Train scatter: [0.1566 0.2195 0.2594 0.399  0.3967 0.1894] \n",
      "         Test scatter: [0.1489 0.2197 0.249  0.3946 0.3922 0.1862]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████▌                                              | 50/200 [02:01<06:30,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Train scatter: [0.1559 0.2143 0.2572 0.4006 0.3986 0.1871] \n",
      "         Test scatter: [0.1479 0.2144 0.2458 0.3953 0.3929 0.1839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████▌                                           | 60/200 [02:24<06:04,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 060, Train scatter: [0.1557 0.2156 0.2579 0.4004 0.3982 0.1878] \n",
      "         Test scatter: [0.1476 0.2159 0.2465 0.3959 0.3934 0.1848]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████▋                                        | 70/200 [02:48<05:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 070, Train scatter: [0.1549 0.2139 0.2563 0.3998 0.3975 0.1888] \n",
      "         Test scatter: [0.1472 0.2143 0.2456 0.3953 0.3929 0.1856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████▊                                     | 80/200 [03:11<05:09,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 080, Train scatter: [0.1549 0.2132 0.2565 0.3976 0.3954 0.1867] \n",
      "         Test scatter: [0.1473 0.2135 0.2456 0.3932 0.3909 0.1838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████▉                                  | 90/200 [03:35<04:46,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 090, Train scatter: [0.1549 0.2134 0.2564 0.399  0.3968 0.1867] \n",
      "         Test scatter: [0.1467 0.214  0.2446 0.3942 0.3918 0.1839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████▌                              | 100/200 [03:58<04:19,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Train scatter: [0.1557 0.2132 0.2567 0.3987 0.3966 0.1878] \n",
      "         Test scatter: [0.1477 0.2135 0.2453 0.3943 0.392  0.1846]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████▌                           | 110/200 [04:21<03:52,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Train scatter: [0.1551 0.2134 0.2566 0.401  0.3989 0.1885] \n",
      "         Test scatter: [0.1474 0.2135 0.2457 0.3948 0.3924 0.1856]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████▌                        | 120/200 [04:45<03:27,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120, Train scatter: [0.1553 0.2161 0.257  0.3985 0.3963 0.187 ] \n",
      "         Test scatter: [0.1472 0.2165 0.2451 0.3932 0.3908 0.1841]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████████████████▋                     | 130/200 [05:08<03:01,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130, Train scatter: [0.1545 0.2129 0.2553 0.3979 0.3957 0.1867] \n",
      "         Test scatter: [0.1465 0.2131 0.2439 0.393  0.3906 0.1836]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████▋                  | 140/200 [05:32<02:36,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140, Train scatter: [0.1546 0.2134 0.2549 0.3988 0.3966 0.1868] \n",
      "         Test scatter: [0.1471 0.2137 0.2441 0.3936 0.3911 0.1838]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████▊               | 150/200 [05:55<02:09,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Train scatter: [0.1541 0.2131 0.2546 0.3985 0.3961 0.1868] \n",
      "         Test scatter: [0.1463 0.2136 0.2435 0.3941 0.3915 0.184 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████▊            | 160/200 [06:19<01:43,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Train scatter: [0.155  0.2123 0.2564 0.3976 0.3954 0.1867] \n",
      "         Test scatter: [0.1475 0.2126 0.2456 0.3937 0.3914 0.1839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████▊         | 170/200 [06:42<01:17,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170, Train scatter: [0.1541 0.213  0.255  0.3976 0.3954 0.1861] \n",
      "         Test scatter: [0.1462 0.2133 0.2436 0.3927 0.3903 0.1832]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████▉      | 180/200 [07:06<00:52,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180, Train scatter: [0.1552 0.2122 0.2563 0.3988 0.3966 0.187 ] \n",
      "         Test scatter: [0.1473 0.213  0.245  0.3949 0.3926 0.184 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████▉   | 190/200 [07:34<00:31,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Train scatter: [0.1546 0.2109 0.2555 0.3982 0.3959 0.1861] \n",
      "         Test scatter: [0.1469 0.2116 0.2442 0.3933 0.391  0.183 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 200/200 [07:57<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Train scatter: [0.1551 0.2111 0.2557 0.3991 0.3968 0.1867] \n",
      "         Test scatter: [0.1477 0.2122 0.245  0.3955 0.3931 0.1835]\n",
      "477.89 seconds spent training, 2.389 seconds per epoch. Processed 36370 trees per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs=200\n",
    "\n",
    "tr_acc, te_acc = [], []\n",
    "start=time.time()\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "#     print(epoch)\n",
    "    train()\n",
    "    if (epoch+1)%10==0:\n",
    "        train_acc, _ , _ = test(train_loader)\n",
    "        test_acc, _ , _ = test(test_loader)\n",
    "        tr_acc.append(train_acc.cpu())\n",
    "        te_acc.append(test_acc.cpu())\n",
    "        print(f'Epoch: {epoch+1:03d}, Train scatter: {np.round(train_acc.cpu().numpy(), 4)} \\n \\\n",
    "        Test scatter: {np.round(test_acc.cpu().numpy(), 4)}')\n",
    "stop=time.time()\n",
    "spent=stop-start\n",
    "print(f\"{spent:.2f} seconds spent training, {spent/n_epochs:.3f} seconds per epoch. Processed {len(ys)*split*n_epochs/spent:.0f} trees per second\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8398be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LayerNorm in module torch.nn.modules.normalization object:\n",
      "\n",
      "class LayerNorm(torch.nn.modules.module.Module)\n",
      " |  LayerNorm(normalized_shape: Union[int, List[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, device=None, dtype=None) -> None\n",
      " |  \n",
      " |  Applies Layer Normalization over a mini-batch of inputs as described in\n",
      " |  the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__\n",
      " |  \n",
      " |  .. math::\n",
      " |      y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
      " |  \n",
      " |  The mean and standard-deviation are calculated separately over the last\n",
      " |  certain number dimensions which have to be of the shape specified by\n",
      " |  :attr:`normalized_shape`.\n",
      " |  :math:`\\gamma` and :math:`\\beta` are learnable affine transform parameters of\n",
      " |  :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.\n",
      " |  The standard-deviation is calculated via the biased estimator, equivalent to\n",
      " |  `torch.var(input, unbiased=False)`.\n",
      " |  \n",
      " |  .. note::\n",
      " |      Unlike Batch Normalization and Instance Normalization, which applies\n",
      " |      scalar scale and bias for each entire channel/plane with the\n",
      " |      :attr:`affine` option, Layer Normalization applies per-element scale and\n",
      " |      bias with :attr:`elementwise_affine`.\n",
      " |  \n",
      " |  This layer uses statistics computed from input data in both training and\n",
      " |  evaluation modes.\n",
      " |  \n",
      " |  Args:\n",
      " |      normalized_shape (int or list or torch.Size): input shape from an expected input\n",
      " |          of size\n",
      " |  \n",
      " |          .. math::\n",
      " |              [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n",
      " |                  \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n",
      " |  \n",
      " |          If a single integer is used, it is treated as a singleton list, and this module will\n",
      " |          normalize over the last dimension which is expected to be of that specific size.\n",
      " |      eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
      " |      elementwise_affine: a boolean value that when set to ``True``, this module\n",
      " |          has learnable per-element affine parameters initialized to ones (for weights)\n",
      " |          and zeros (for biases). Default: ``True``.\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(N, *)`\n",
      " |      - Output: :math:`(N, *)` (same shape as input)\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> input = torch.randn(20, 5, 10, 10)\n",
      " |      >>> # With Learnable Parameters\n",
      " |      >>> m = nn.LayerNorm(input.size()[1:])\n",
      " |      >>> # Without Learnable Parameters\n",
      " |      >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n",
      " |      >>> # Normalize over last two dimensions\n",
      " |      >>> m = nn.LayerNorm([10, 10])\n",
      " |      >>> # Normalize over last dimension of size 10\n",
      " |      >>> m = nn.LayerNorm(10)\n",
      " |      >>> # Activating the module\n",
      " |      >>> output = m(input)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LayerNorm\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, normalized_shape: Union[int, List[int], torch.Size], eps: float = 1e-05, elementwise_affine: bool = True, device=None, dtype=None) -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'elementwise_affine': <class 'bool'>, 'eps': <class...\n",
      " |  \n",
      " |  __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |          or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.mlp[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e17eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "today = today.strftime(\"%d%m%y\")\n",
    "\n",
    "torch.save(model.state_dict(),f'trained_models/model_{epoch}_{today}.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10fd0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load=False\n",
    "if load:\n",
    "    model = GCN(hidden_channelse=128)\n",
    "    model.load_state_dict(torch.load(f'trained_models', f'model_{epoch}_{date}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6c81daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder tensor(1.0244, grad_fn=<MulBackward0>)\n",
      "encoder tensor(1.0797, grad_fn=<MulBackward0>)\n",
      "edge tensor(0.5381, grad_fn=<MulBackward0>)\n",
      "both tensor(1.1271, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('decoder', sum(p.abs().sum() for p in model.f.parameters())/sum(p.numel() for p in model.f.parameters())*100)\n",
    "\n",
    "print('encoder', sum(p.abs().sum() for p in model.g1.parameters())/sum(p.numel() for p in model.g1.parameters())*100)\n",
    "print('edge', sum(p.abs().sum() for p in model.g2.parameters())/sum(p.numel() for p in model.g2.parameters())*100)\n",
    "print('both', sum(p.abs().sum() for p in model.g3.parameters())/sum(p.numel() for p in model.g3.parameters())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7371f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(osp.join(pointer, model_runs[k], 'trained_model', 'model.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d621ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pysr_loader=DataLoader(test_data[:50], batch_size=batch_size, shuffle=0, num_workers=4)    \n",
    "\n",
    "y_f = []\n",
    "y_g1 = []\n",
    "y_g2 = []\n",
    "y_g3 = []\n",
    "x_g1 = []\n",
    "y_t = []\n",
    "batch = []\n",
    "i=0\n",
    "for dat in pysr_loader:\n",
    "    print(i)\n",
    "    x_g1.append(dat.x.cpu().detach().numpy())\n",
    "    yg1=model.g1(dat.x)\n",
    "    y_g1.append(yg1.cpu().detach().numpy())\n",
    "    \n",
    "    adj = dat.edge_index\n",
    "    neighbours = yg1\n",
    "    xg2 = scatter_add(neighbours[adj[0]], adj[1], dim=0)\n",
    "    x_g2.append(xg2)\n",
    "    yg2=model.g2(xg2)\n",
    "    y_g2.append(yg2.cpu().detach().numpy())\n",
    "    \n",
    "    yg1[adj[1]]+=yg2[adj[1]]\n",
    "    \n",
    "    yg3 = model.g3(yg1)\n",
    "    y_g3.append(yg3.cpu().detach().numpy())\n",
    "    x_f = global_add_pool(yg3, dat.batch)\n",
    "    \n",
    "    yf = model.f(x_f)\n",
    "    y_f.append(yf.cpu().detach().numpy())\n",
    "    \n",
    "    batch.append(dat.batch.cpu().detach().numpy())\n",
    "    y_t.append(dat.y.cpu().detach().numpy())\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e6d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=25\n",
    "vals, counts = np.unique(batch[0], return_counts=1)\n",
    "l = np.cumsum(counts)[N-1]\n",
    "x_g1_pysr=np.vstack(x_g1[0][:l])\n",
    "y_g1_pysr=np.vstack(y_g1[0][:l])\n",
    "y_g2_pysr=np.vstack(y_g2[0][:l])\n",
    "y_g3_pysr=np.vstack(y_g3[0][:l])\n",
    "y_f_pysr=np.vstack(y_f[0][:l])\n",
    "\n",
    "b_pysr = batch[0][:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92fb4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on julia -O3 --threads 4 /tmp/tmpyc0xp9yp/runfile.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "\n",
      "Cycles per second: 6.440e+03\n",
      "Head worker occupation: 6.6%\n",
      "Progress: 3 / 1200 total iterations (0.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           9.573e-01  7.446e-02  (-0.39906436 + (2.5171049 * x1))\n",
      "7           7.673e-01  1.106e-01  (-0.2528663 + (x4 + (2.1287289 * x1)))\n",
      "8           7.581e-01  1.199e-02  ((2.0636811 * x1) + log_abs(0.94439185 + x4))\n",
      "9           6.789e-01  1.104e-01  (x1 + ((-0.57546425 * x0) + (3.2942095 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71885884 + (2.645293 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.491e+00  1.412e-01  ((2.6945295 * x1) + log_abs(-0.72173405 / x2))\n",
      "9           4.379e+00  2.510e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.017e+00  8.639e-02  ((2.9049654 * x1) + log_abs(sqrt_abs(log_abs(x2)) / x2))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.110e+03\n",
      "Head worker occupation: 2.9%\n",
      "Progress: 4 / 1200 total iterations (0.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765393 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25284228 + (x4 + (2.128747 * x1)))\n",
      "8           7.581e-01  1.199e-02  ((2.0636811 * x1) + log_abs(0.94439185 + x4))\n",
      "9           6.986e-01  8.170e-02  (x4 + ((-0.35670218 * x3) + (1.9270613 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71885884 + (2.645293 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.491e+00  1.412e-01  ((2.6945295 * x1) + log_abs(-0.72173405 / x2))\n",
      "9           4.379e+00  2.510e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.017e+00  8.639e-02  ((2.9049654 * x1) + log_abs(sqrt_abs(log_abs(x2)) / x2))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.930e+03\n",
      "Head worker occupation: 1.8%\n",
      "Progress: 7 / 1200 total iterations (0.583%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347527 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765505 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25280938 + (x4 + (2.128747 * x1)))\n",
      "9           7.660e-01  8.083e-04  (-0.24002542 + ((1.0874321 * x4) + (2.0947928 * x1)))\n",
      "10          7.295e-01  4.893e-02  (x1 - ((x4 * (log_abs(x4) + -1.0388708)) - x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7933848 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188488 + (2.6452794 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.431e+00  1.547e-01  (x0 + (-3.369158 * (x1 * log_abs(x2))))\n",
      "9           4.379e+00  1.168e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.305e+00  1.720e-02  (x0 + (2.9537868 * (x1 * log_abs(1.1906661 / x2))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.860e+03\n",
      "Head worker occupation: 1.3%\n",
      "Progress: 10 / 1200 total iterations (0.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979004 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "7           6.145e-01  2.423e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.867392 + (x1 + (-2.314182 * sqrt_abs(x2))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7936423 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188475 + (2.645304 * x1))\n",
      "6           5.024e+00  2.188e-01  ((x1 / 0.36871853) - log_abs(x1))\n",
      "7           4.676e+00  7.183e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.509e+00  3.631e-02  ((x1 / 0.37334132) - log_abs(x2 * 1.2066805))\n",
      "9           4.318e+00  4.324e-02  ((x1 / 0.34495905) - log_abs(sqrt_abs(x2) * x1))\n",
      "10          4.201e+00  2.752e-02  ((8.892884 * x1) + (-10.245522 * (x2 * sqrt_abs(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.610e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 11 / 1200 total iterations (0.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7936423 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188475 + (2.645304 * x1))\n",
      "6           5.024e+00  2.188e-01  ((x1 / 0.36871853) - log_abs(x1))\n",
      "7           4.676e+00  7.183e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.509e+00  3.631e-02  ((x1 / 0.37334132) - log_abs(x2 * 1.2066805))\n",
      "9           4.318e+00  4.324e-02  ((x1 / 0.34495905) - log_abs(sqrt_abs(x2) * x1))\n",
      "10          4.201e+00  2.752e-02  ((8.892884 * x1) + (-10.245522 * (x2 * sqrt_abs(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 3.380e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 13 / 1200 total iterations (1.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71897423 + (2.6452804 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 - log_abs(x2)) + x1)\n",
      "7           4.676e+00  4.954e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.560e+00  2.520e-02  ((x1 - (log_abs(x2) - x0)) + x1)\n",
      "10          4.217e+00  3.903e-02  (-0.93081266 + ((-1.5788529 * log_abs(x2)) + (2.7227342 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.080e+03\n",
      "Head worker occupation: 0.8%\n",
      "Progress: 14 / 1200 total iterations (1.167%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7940874 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188683 + (2.6452758 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 - log_abs(x2)) + x1)\n",
      "8           4.491e+00  4.498e-02  ((2.6943352 * x1) + log_abs(0.7219004 / x2))\n",
      "10          3.967e+00  6.198e-02  ((3.6765597 * x1) + log_abs(-0.8480846 + (0.8631841 / x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.330e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 17 / 1200 total iterations (1.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3798684 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.95245224 * log10_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.965e-01  3.125e-02  (x1 + (-0.93270797 * log10_abs(0.06882628 + x2)))\n",
      "9           5.494e-01  8.224e-02  (x1 - log10_abs((x3 * log10_abs(x0)) + x2))\n",
      "10          5.306e-01  3.486e-02  (x1 - log10_abs((x3 * log10_abs(sqrt_abs(x0))) + x2))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71884656 + (2.6452959 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "7           4.044e+00  1.946e-01  ((-9.898346 * x2) + (8.19939 * x1))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.100e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 18 / 1200 total iterations (1.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3798684 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.95245224 * log10_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.965e-01  3.125e-02  (x1 + (-0.93270797 * log10_abs(0.06882628 + x2)))\n",
      "9           5.494e-01  8.224e-02  (x1 - log10_abs((x3 * log10_abs(x0)) + x2))\n",
      "10          5.306e-01  3.486e-02  (x1 - log10_abs((x3 * log10_abs(sqrt_abs(x0))) + x2))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.150e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 20 / 1200 total iterations (1.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37978694 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41363257 * log_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.981e-01  2.865e-02  (x1 + (-0.92408365 * log10_abs(0.05345475 + x2)))\n",
      "9           5.918e-01  1.052e-02  (x1 - (log10_abs(x2) * sqrt_abs(x0 + 1.1087832)))\n",
      "10          5.659e-01  4.476e-02  (x1 + (-1.07813 * log10_abs(x2 + (-0.09936239 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.076545 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25267908 + (x4 + (2.1288564 * x1)))\n",
      "9           6.693e-01  6.832e-02  (-0.5448855 + ((-0.5488192 * x3) + (2.3172386 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 2.680e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 21 / 1200 total iterations (1.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.990e-01  8.240e-02  (x1 + 0.34660563)\n",
      "4           6.190e-01  2.553e-01  (x1 - log10_abs(x2))\n",
      "7           5.908e-01  1.554e-02  ((2.9686167 * x1) + (-3.5632546 * x2))\n",
      "8           5.679e-01  3.962e-02  ((2.3849034 - log_abs(x1)) * (x1 - x2))\n",
      "10          5.659e-01  1.701e-03  (x1 + (-1.07813 * log10_abs(x2 + (-0.09936239 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.076545 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25267908 + (x4 + (2.1288564 * x1)))\n",
      "9           6.693e-01  6.832e-02  (-0.5448855 + ((-0.5488192 * x3) + (2.3172386 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.990e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 24 / 1200 total iterations (2.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979323 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.143e-01  3.843e-03  (x1 - log10_abs(x2 * 1.2160699))\n",
      "8           5.966e-01  1.461e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "10          5.663e-01  2.607e-02  (x1 - log10_abs((x2 + (x3 * -0.09657265)) * -1.0948982))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765564 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.2530826 + (x4 + (2.1288114 * x1)))\n",
      "9           6.986e-01  4.684e-02  (x4 + ((-0.35670036 * x3) + (1.9270647 * x1)))\n",
      "10          6.585e-01  5.921e-02  (x4 + ((3.4497137 * x2) + (x3 * log10_abs(x0))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.770e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 25 / 1200 total iterations (2.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979323 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.143e-01  3.843e-03  (x1 - log10_abs(x2 * 1.2160699))\n",
      "8           5.966e-01  1.461e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "10          5.663e-01  2.607e-02  (x1 - log10_abs((x2 + (x3 * -0.09657265)) * -1.0948982))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.131e+00  4.509e-01  (2.5918505 * x1)\n",
      "4           1.117e+00  1.246e-02  (sqrt_abs(5.5143814) * x1)\n",
      "5           8.352e-01  2.909e-01  ((x1 * 1.9893233) + x4)\n",
      "7           7.673e-01  4.239e-02  (-0.25282595 + (x4 + (2.128735 * x1)))\n",
      "8           6.761e-01  1.265e-01  (x4 + (x1 * sqrt_abs(4.6921225 / x1)))\n",
      "10          6.077e-01  5.331e-02  (x2 + (x4 + (x1 * log_abs(5.268682 / x1))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.750e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 27 / 1200 total iterations (2.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797861 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41367182 * log_abs(x2)))\n",
      "8           5.966e-01  1.780e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.265e+00  3.950e-01  (x1 + x1)\n",
      "4           1.111e+00  1.295e-01  (x1 * sqrt_abs(6.014003))\n",
      "5           8.822e-01  2.308e-01  ((x1 * 3.2053354) - x0)\n",
      "6           8.366e-01  5.304e-02  (x1 * sqrt_abs(-6.5326552 / x1))\n",
      "7           7.727e-01  7.943e-02  ((-0.6377825 * x0) + (2.913713 * x1))\n",
      "8           6.761e-01  1.336e-01  (x4 + (x1 * sqrt_abs(4.6921225 / x1)))\n",
      "9           6.611e-01  2.232e-02  (-0.3417246 + ((-0.59988934 * x0) + (2.9557881 * x1)))\n",
      "10          6.077e-01  8.430e-02  (x2 + (x4 + (x1 * log_abs(5.268682 / x1))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.620e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 28 / 1200 total iterations (2.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797861 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41367182 * log_abs(x2)))\n",
      "8           5.966e-01  1.780e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.232e+00  4.080e-01  (x1 / 0.48811185)\n",
      "5           8.337e-01  1.954e-01  ((x1 + x4) + x1)\n",
      "8           8.279e-01  2.349e-03  (x1 - ((x3 - x1) + sqrt_abs(x2)))\n",
      "9           6.693e-01  2.127e-01  (-0.5449041 + ((-0.5488431 * x3) + (2.317193 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 2.770e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 30 / 1200 total iterations (2.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972298 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85042274 / x2))\n",
      "8           6.078e-01  5.104e-03  (x1 + log10_abs(-0.3060487 + (0.8593486 / x2)))\n",
      "9           5.971e-01  1.785e-02  ((x1 - log10_abs(x2 * sqrt_abs(x1))) - 0.18158987)\n",
      "10          5.677e-01  5.047e-02  (x1 + log10_abs((-1.0497826 / x2) + (-0.7018149 * x3)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.232e+00  4.080e-01  (x1 / 0.48811185)\n",
      "5           8.337e-01  1.954e-01  ((x1 + x4) + x1)\n",
      "8           8.279e-01  2.349e-03  (x1 - ((x3 - x1) + sqrt_abs(x2)))\n",
      "9           6.693e-01  2.127e-01  (-0.5449041 + ((-0.5488431 * x3) + (2.317193 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "3           7.265e+00  1.273e-01  (x1 + x1)\n",
      "4           6.749e+00  7.369e-02  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71886116 + (2.6453934 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "8           4.560e+00  3.737e-02  (((x0 + x1) + x1) - log_abs(x2))\n",
      "10          4.217e+00  3.903e-02  (-0.9306045 + ((-1.5786276 * log_abs(x2)) + (2.7231658 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.210e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 33 / 1200 total iterations (2.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972295 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85011387 / x2))\n",
      "7           5.908e-01  3.858e-02  ((-3.5634954 * x2) + (2.96877 * x1))\n",
      "9           5.882e-01  2.222e-03  (-0.073791474 + ((3.1524632 * x1) + (-3.8719606 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347527 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.07655 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.2528399 + (x4 + (2.128747 * x1)))\n",
      "9           7.532e-01  9.242e-03  (-0.1331652 + (x2 + (x4 + (1.5579417 * x1))))\n",
      "10          6.607e-01  1.311e-01  (x4 + ((1.8001357 * x1) + log10_abs(0.34272045 + x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "3           7.265e+00  1.273e-01  (x1 + x1)\n",
      "4           6.749e+00  7.369e-02  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71886116 + (2.6453934 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "8           4.560e+00  3.737e-02  (((x0 + x1) + x1) - log_abs(x2))\n",
      "10          4.217e+00  3.903e-02  (-0.9306045 + ((-1.5786276 * log_abs(x2)) + (2.7231658 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.070e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 36 / 1200 total iterations (3.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972295 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85011387 / x2))\n",
      "7           5.908e-01  3.858e-02  ((-3.5634954 * x2) + (2.96877 * x1))\n",
      "9           5.882e-01  2.222e-03  (-0.073791474 + ((3.1524632 * x1) + (-3.8719606 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.793656 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188273 + (2.6452773 * x1))\n",
      "6           5.172e+00  1.898e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  ((x1 + (x0 + x4)) - log_abs(x2))\n",
      "10          4.112e+00  4.857e-02  (((x1 + x1) - log_abs(x1 - 0.2593729)) / 0.6445904)\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.480e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 39 / 1200 total iterations (3.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797341 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(-0.8504216 / x2))\n",
      "7           6.119e-01  3.561e-03  (x1 + sqrt_abs(log10_abs(-0.36682373 + x1)))\n",
      "9           5.966e-01  1.267e-02  (x1 + log10_abs(log10_abs(0.15950276 * x3) / x2))\n",
      "10          5.824e-01  2.393e-02  (x1 + log10_abs(-0.90379727 / (x2 + (0.115976155 * x0))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.793656 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188273 + (2.6452773 * x1))\n",
      "6           5.172e+00  1.898e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  ((x1 + (x0 + x4)) - log_abs(x2))\n",
      "10          4.112e+00  4.857e-02  (((x1 + x1) - log_abs(x1 - 0.2593729)) / 0.6445904)\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.480e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 42 / 1200 total iterations (3.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37967104 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "8           5.698e-01  2.072e-02  (1.8674098 + (x1 + (-2.314192 * sqrt_abs(x2))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188429 + (2.6452808 * x1))\n",
      "6           4.913e+00  2.411e-01  (x1 + (x1 - log_abs(x2)))\n",
      "8           4.679e+00  2.442e-02  (x1 + (x1 - log_abs(x1 - 0.45799425)))\n",
      "10          3.656e+00  1.234e-01  (x0 + (x1 + (-1.9220711 * log_abs(-0.39283225 + x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing process... will return when done.\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "g1_equations = pysr(\n",
    "    X=x_g1_pysr, y=y_g1_pysr,\n",
    "    procs=4,\n",
    "    niterations=20,\n",
    "    populations=20,\n",
    "    useFrequency=True,\n",
    "    multithreading=True, \n",
    "    binary_operators=[\"plus\", \"sub\", \"mult\", \"div\"],\n",
    "    unary_operators = ['log10_abs', 'sqrt_abs', 'exp', 'log'], ##still need a general power law\n",
    "    batching=1, \n",
    "    batchSize=256,\n",
    "    maxsize=10, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ef6a182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                                4\n",
       "MSE                                                0.618991\n",
       "score                                              0.253895\n",
       "Equation                               (x1 - log10_abs(x2))\n",
       "sympy_format                      x1 - log(Abs(x2))/log(10)\n",
       "lambda_format    PySRFunction(X=>x1 - log(Abs(x2))/log(10))\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[0].sort_values(by='score', ascending=False).iloc[0]\n",
    "a=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3ce5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                          3\n",
       "MSE                                          1.143048\n",
       "score                                        0.445626\n",
       "Equation                            (x1 / 0.37977126)\n",
       "sympy_format                      2.63316397349289*x1\n",
       "lambda_format    PySRFunction(X=>2.63316397349289*x1)\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[1].sort_values(by='score', ascending=False).iloc[0]\n",
    "b=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "33090e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                   3\n",
       "MSE                                   6.751929\n",
       "score                                 0.298143\n",
       "Equation                      (2.7937005 * x1)\n",
       "sympy_format                      2.7937005*x1\n",
       "lambda_format    PySRFunction(X=>2.7937005*x1)\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[2].sort_values(by='score', ascending=False).iloc[0]\n",
    "c=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5414aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_an_pysr = np.vstack([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838585ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on julia -O3 --threads 4 /tmp/tmpgyms7yzv/runfile.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "\n",
      "Cycles per second: 5.180e+03\n",
      "Head worker occupation: 8.3%\n",
      "Progress: 2 / 1200 total iterations (0.167%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2790511\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.2852532 + (-0.26651594 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.950e+03\n",
      "Head worker occupation: 5.1%\n",
      "Progress: 3 / 1200 total iterations (0.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.440e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 4 / 1200 total iterations (0.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.710e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 5 / 1200 total iterations (0.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.430e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 6 / 1200 total iterations (0.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.285257 + (-0.2665256 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.740e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 8 / 1200 total iterations (0.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.750e+03\n",
      "Head worker occupation: 0.8%\n",
      "Progress: 9 / 1200 total iterations (0.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 3.420e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 10 / 1200 total iterations (0.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.350e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 13 / 1200 total iterations (1.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.450e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 15 / 1200 total iterations (1.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.093e-03  sqrt_abs(1.9594429 + (-0.6206558 * x1))\n",
      "8           5.891e-01  7.699e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.648e-01  2.098e-02  sqrt_abs(0.11480782 + exp(1.308776 + (-0.45345652 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.080e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 16 / 1200 total iterations (1.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2788731\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.648e-01  2.098e-02  sqrt_abs(0.11480782 + exp(1.308776 + (-0.45345652 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.810e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 19 / 1200 total iterations (1.583%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2786434\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.110e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 21 / 1200 total iterations (1.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 5.100e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 22 / 1200 total iterations (1.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.060e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 23 / 1200 total iterations (1.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.730e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 25 / 1200 total iterations (2.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25174072 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852515 + (-0.2665252 * x1))\n",
      "6           5.982e-01  2.083e-03  sqrt_abs(1.9546335 + (-0.6191466 * x1))\n",
      "7           5.891e-01  1.541e-02  exp(0.6601062 + (-0.19049618 * exp(x1)))\n",
      "9           5.862e-01  2.450e-03  exp((x1 + (sqrt_abs(x1) - 1.815573)) * -0.24319673)\n",
      "10          5.570e-01  5.109e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.650e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 28 / 1200 total iterations (2.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25174072 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852579 + (-0.26652578 * x1))\n",
      "6           5.982e-01  2.072e-03  sqrt_abs(1.9627051 + (-0.62186176 * x1))\n",
      "7           5.873e-01  1.847e-02  (sqrt_abs(sqrt_abs(9.484525) - x1) + -0.39439014)\n",
      "8           5.845e-01  4.689e-03  (-0.30447757 + sqrt_abs(2.8036215 + (-0.8870912 * x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.130e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 30 / 1200 total iterations (2.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2789322\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852509 + (-0.26652473 * x1))\n",
      "6           5.982e-01  2.082e-03  sqrt_abs(1.9564418 + (-0.61975557 * x1))\n",
      "7           5.891e-01  1.541e-02  exp(0.6601307 + (-0.19048771 * exp(x1)))\n",
      "8           5.845e-01  7.735e-03  (-0.30447757 + sqrt_abs(2.8036215 + (-0.8870912 * x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.150e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 32 / 1200 total iterations (2.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852594 + (-0.26652542 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 4.950e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 35 / 1200 total iterations (2.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852598 + (-0.26652578 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.910e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 36 / 1200 total iterations (3.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852598 + (-0.26652578 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.030e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 39 / 1200 total iterations (3.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852643 + (-0.26654008 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.870e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 41 / 1200 total iterations (3.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2790481\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517328 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852972 + (-0.26651946 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "7           5.752e-01  1.888e-02  log10_abs(exp(1.9263862 - x1) + x2)\n",
      "8           5.707e-01  7.786e-03  log10_abs(exp(exp(0.74600375) - x1) + x2)\n",
      "9           5.671e-01  6.467e-03  log10_abs(x2 + exp(2.390812 + (-0.8634285 * x1)))\n",
      "10          5.169e-01  9.272e-02  log10_abs(x2 + exp(4.511248 + (-1.0051906 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    }
   ],
   "source": [
    "g2_equations = pysr(\n",
    "    X=y_g1_pysr, y=y_g2_pysr,\n",
    "    procs=4,\n",
    "    niterations=20,\n",
    "    populations=20,\n",
    "    useFrequency=True,\n",
    "    multithreading=True, \n",
    "    binary_operators=[\"plus\", \"sub\", \"mult\", \"div\", 'pow'],\n",
    "    unary_operators = ['log10_abs', 'sqrt_abs', 'exp', 'log'], ##still need a general power law\n",
    "    batching=1, \n",
    "    batchSize=256,\n",
    "    maxsize=10, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aca42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
