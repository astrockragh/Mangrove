{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34bc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, time, os, random\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "# accelerate huggingface to GPU\n",
    "if torch.cuda.is_available():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855fbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_t=['mstar stellar mass [1.0E09 Msun](0)',\n",
    " ' v_disk rotation velocity of disk [km/s] (1)',\n",
    " ' r_bulge 3D effective radius of bulge [kpc](2)',\n",
    " ' mcold cold gas mass in disk [1.0E09 Msun](3)',\n",
    " ' mHI cold gas mass [1.0E09 Msun](4)',\n",
    " ' mH2 cold gas mass [1.0E09 Msun](5)',\n",
    " ' mHII cold gas mass [1.0E09 Msun](6)',\n",
    " ' sfrave100myr SFR averaged over 100 Myr [Msun/yr](7)']\n",
    "\n",
    "cols_t=['mstar stellar mass [1.0E09 Msun](0)',\n",
    " ' v_disk rotation velocity of disk [km/s] (1)',\n",
    " ' mcold cold gas mass in disk [1.0E09 Msun](3)',\n",
    " ' sfrave100myr SFR averaged over 100 Myr [Msun/yr](7)']\n",
    "\n",
    "all_cols=np.array([0,2,4,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,35]+list(range(37,60)))\n",
    "targets=[8,11,14,15,16,17,18,23]\n",
    "targets=[8,11,15,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5ddb58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vlarge_all_4t_z1.0_standard_quant',\n",
       " 'vlarge_all_4t_z0.3_quantile_raw',\n",
       " 'vlarge_4t_quantile_raw_redshift_75_all',\n",
       " 'vlarge_all_4t_z1.0_quantile_raw',\n",
       " 'vlarge_all_4t_z0.3_None',\n",
       " 'vlarge_all_4t_z3.0_quantile_raw',\n",
       " 'vlarge_all_4t_z2.0_standard_quant',\n",
       " 'vlarge_all_4t_z0.8_quantile_raw',\n",
       " 'tvt_idx',\n",
       " 'vlarge_all_4t_z2.0_None',\n",
       " 'redshift_scan_0',\n",
       " 'testid_all_4t_z2.0_None',\n",
       " 'vlarge_all_4t_z0.0_quantile_stand',\n",
       " 'vlarge_all_multi_try1',\n",
       " 'vlarge_4t_quantile_raw_redshift_99_all',\n",
       " 'vlarge_all_4t_z2.0_quantile_raw',\n",
       " 'vlarge_all_4t_z0.0_standard_quant',\n",
       " 'vlarge_all_4t_z0.5_quantile_quant',\n",
       " 'vlarge_4t_quantile_raw_redshift_50_all',\n",
       " 'vlarge_all_4t_z2.0_quantile_stand',\n",
       " 'vlarge_all_4t_z1.0_quantile_quant',\n",
       " 'transformers',\n",
       " 'vlarge_all_4t_z0.0_standard_raw',\n",
       " 'vlarge_all_4t_quantile_raw_final',\n",
       " 'vlarge_all_4t_z0.5_standard_stand',\n",
       " 'vlarge_all_4t_z1.8_quantile_raw',\n",
       " 'vlarge_all_4t_z0.5_standard_quant',\n",
       " 'vlarge_all_4t_zall_quantile_raw_trainandtest',\n",
       " 'vlarge_all_4t_z0.0_quantile_raw',\n",
       " 'old',\n",
       " 'vlarge_all_4t_z1.0_None',\n",
       " 'vlarge_all_4t_z1.5_quantile_raw',\n",
       " 'vlarge_all_4t_z1.0_standard_stand',\n",
       " 'vlarge_all_4t_z0.8_None',\n",
       " 'vlarge_all_4t_z1.8_None',\n",
       " 'vlarge_all_4t_z2.0_standard_raw',\n",
       " 'vlarge_4t_quantile_raw_redshift_95_all',\n",
       " 'testid_all_4t_z0.0_None',\n",
       " 'vlarge_all_4t_z3.0_None',\n",
       " 'vlarge_all_4t_z0.5_standard_raw',\n",
       " 'vlarge_all_4t_z1.5_None',\n",
       " 'vlarge_all_4t_z0.0_None',\n",
       " 'vlarge_4t_quantile_raw_redshift_85_all',\n",
       " 'vlarge_all_4t_z0.5_quantile_raw',\n",
       " 'vlarge_all_4t_z1.0_standard_raw',\n",
       " 'vlarge_all_4t_quantile_raw',\n",
       " 'testt_all_4t_z0.0_None',\n",
       " 'vlarge_all_smass',\n",
       " 'vlarge_all_4t_z0.0_quantile_quant',\n",
       " 'vlarge_all_4t_z0.5_quantile_stand',\n",
       " 'vlarge_all_4t_zall_quantile_raw',\n",
       " 'vlarge_all_4t_z0.0_standard_stand',\n",
       " 'vlarge_all_4t_z1.0_quantile_stand',\n",
       " 'vlarge_all_4t_z2.0_quantile_quant',\n",
       " 'vlarge_all_4t_z2.0_standard_stand',\n",
       " 'vlarge_all_4t_z0.5_None']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(osp.expanduser('~/../../../scratch/gpfs/cj1223/GraphStorage/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f45d75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='vlarge_all_multi_try1/vlarge_all_multisimple_z0.0_quantile_raw'\n",
    "case='vlarge_all_4t_z0.0_quantile_raw'\n",
    "\n",
    "\n",
    "\n",
    "data=pickle.load(open(osp.expanduser(f'~/../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db8a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Data\n",
    "\n",
    "# cols_t=['mstar stellar mass [1.0E09 Msun](0)',\n",
    "#  ' mcold cold gas mass in disk [1.0E09 Msun](3)']\n",
    "# cols_t=['mstar stellar mass [1.0E09 Msun](0)',\n",
    "#   ' v_disk rotation velocity of disk [km/s] (1)']\n",
    "# targets=[8,15]\n",
    "# data=[]\n",
    "# for d in datat:\n",
    "#     data.append(Data(x=d.x, edge_index=d.edge_index, edge_attr=d.edge_attr, y=d.y[[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f2cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=[]\n",
    "for d in data:\n",
    "    ys.append(d.y.numpy())\n",
    "ys=np.vstack(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e90c231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.89590685],\n",
       "       [0.89590685, 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ys[:,0],ys[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926b62a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_targ=len(data[0].y)\n",
    "n_feat=len(data[0].x[0])\n",
    "n_feat, n_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78e0a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, LayerNorm, LeakyReLU, Module, ReLU, Sequential, ModuleList\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, norm, global_max_pool, global_add_pool, MetaLayer\n",
    "from torch_scatter import scatter_mean, scatter_sum, scatter_max, scatter_min\n",
    "from torch import cat, square,zeros, clone, abs, sigmoid, float32, tanh\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, n_in, n_out, hidden=64, nlayers=2, layer_norm=True):\n",
    "        super().__init__()\n",
    "        '''Simple two_layer MLP class with ReLU activiation + layernorm to use later'''\n",
    "        layers = [Linear(n_in, hidden), ReLU()]\n",
    "        for i in range(nlayers):\n",
    "            layers.append(Linear(hidden, hidden))\n",
    "            layers.append(ReLU()) \n",
    "        if layer_norm:\n",
    "            layers.append(LayerNorm(hidden))\n",
    "        layers.append(Linear(hidden, n_out))\n",
    "        self.mlp = Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class Sage(Module):\n",
    "    def __init__(self, hidden_channels=128, in_channels=43, out_channels=4, encode=True, conv_layers=3, conv_activation='relu', \n",
    "                    decode_layers=2, decode_activation='leakyrelu', layernorm=True, variance=True, agg='sum', rho=6):\n",
    "        super(Sage, self).__init__()\n",
    "        '''Model built upon the GraphSAGE convolutional layer. This is a node only model (no global, no edge).\n",
    "        Model takes a data object from a dataloader in the forward call and takes out the rest itself. \n",
    "        hidden_channels, n_in, n_out must be specified\n",
    "        Most other things can be customized at wish, e.g. activation functions for which ReLU and LeakyReLU can be used'''\n",
    "        self.encode=encode\n",
    "        if self.encode:\n",
    "            self.node_enc = MLP(in_channels, hidden_channels, layer_norm=True)\n",
    "        self.decode_activation=decode_activation\n",
    "        self.conv_activation=conv_activation\n",
    "        self.layernorm=layernorm\n",
    "        self.in_channels=in_channels\n",
    "        self.out_channels=out_channels\n",
    "        self.hidden_channels=hidden_channels\n",
    "        self.variance=variance\n",
    "        self.agg=agg\n",
    "        self.rho=rho\n",
    "        ########################\n",
    "        # Convolutional Layers #\n",
    "        ######################## \n",
    "\n",
    "        self.convs=ModuleList()\n",
    "        if self.encode:\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        else:\n",
    "            self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(int(conv_layers-1)):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        ##################\n",
    "        # Decode Layers #\n",
    "        ##################       \n",
    "\n",
    "        self.decoders = ModuleList()\n",
    "        self.norms = ModuleList()\n",
    "        for _ in range(out_channels):\n",
    "            self.decoder=ModuleList()\n",
    "            self.norm=ModuleList()\n",
    "            for i in range(decode_layers):\n",
    "                if i==decode_layers-1: ## if final layer, make layer with only one output\n",
    "                    self.norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                    self.decoder.append(Linear(hidden_channels, 1))\n",
    "                else:\n",
    "                    self.norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                    self.decoder.append(Linear(hidden_channels, hidden_channels))\n",
    "            self.decoders.append(self.decoder)\n",
    "            self.norms.append(self.norm)\n",
    "\n",
    "        ###################\n",
    "        # Variance Layers #\n",
    "        ###################\n",
    "\n",
    "        if variance:\n",
    "            self.sigs = ModuleList()\n",
    "            self.sig_norms = ModuleList()\n",
    "            for _ in range(out_channels):\n",
    "                self.sig=ModuleList()\n",
    "                self.sig_norm=ModuleList()\n",
    "                for i in range(decode_layers):\n",
    "                    if i==decode_layers-1:\n",
    "                        self.sig_norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                        self.sig.append(Linear(hidden_channels, 1))\n",
    "                    else:\n",
    "                        self.sig_norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                        self.sig.append(Linear(hidden_channels, hidden_channels))\n",
    "                self.sigs.append(self.sig)\n",
    "                self.sig_norms.append(self.sig_norm)\n",
    "\n",
    "        ######################\n",
    "        # Co-Variance Layers #\n",
    "        ######################\n",
    "\n",
    "        if self.rho!=0:\n",
    "            self.rhos = ModuleList()\n",
    "            self.rho_norms = ModuleList()\n",
    "            for _ in range(self.rho):\n",
    "                self.rho_l=ModuleList()\n",
    "                self.rho_norm=ModuleList()\n",
    "                for i in range(decode_layers):\n",
    "                    if i==decode_layers-1:\n",
    "                        self.rho_norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                        self.rho_l.append(Linear(hidden_channels, 1))\n",
    "                    else:\n",
    "                        self.rho_norm.append(LayerNorm(normalized_shape=hidden_channels))\n",
    "                        self.rho_l.append(Linear(hidden_channels, hidden_channels))\n",
    "                self.rhos.append(self.rho_l)\n",
    "                self.rho_norms.append(self.rho_norm)\n",
    "        \n",
    "        #####################\n",
    "        # Activation Layers #\n",
    "        #####################\n",
    "        \n",
    "        self.conv_act=self.conv_act_f()\n",
    "        self.decode_act=self.decode_act_f() ## could apply later\n",
    "\n",
    "    def conv_act_f(self):\n",
    "        if self.conv_activation =='relu':\n",
    "            print('RelU conv activation')\n",
    "            act = ReLU()\n",
    "            return act\n",
    "        if self.conv_activation =='leakyrelu':\n",
    "            print('LeakyRelU conv activation')\n",
    "            act=LeakyReLU()\n",
    "            return act\n",
    "        if not self.conv_activation:\n",
    "            raise ValueError(\"Please specify a conv activation function\")\n",
    "\n",
    "    def decode_act_f(self):\n",
    "        if self.decode_activation =='relu':\n",
    "            print('RelU decode activation')\n",
    "            act = ReLU()\n",
    "            return act\n",
    "        if self.decode_activation =='leakyrelu':\n",
    "            print('LeakyRelU decode activation')\n",
    "            act=LeakyReLU()\n",
    "            return act\n",
    "        if not self.decode_activation:\n",
    "            print(\"Please specify a decode activation function\")\n",
    "            return None\n",
    "\n",
    "    def forward(self, graph):\n",
    "\n",
    "        #get the data\n",
    "        x, edge_index, batch = graph.x, graph.edge_index, graph.batch\n",
    "        if self.encode:\n",
    "            x = self.node_enc(x)\n",
    "        \n",
    "        #convolutions \n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x=self.conv_act(x)\n",
    "        if self.agg=='sum':\n",
    "            x = global_add_pool(x, batch)\n",
    "        if self.agg=='max':\n",
    "            x = global_max_pool(x, batch)\n",
    "        \n",
    "        #decoder\n",
    "        \n",
    "        x_out=[]\n",
    "        for norm, decode in zip(self.norms, self.decoders):\n",
    "            x1=clone(x)\n",
    "            for n, d in zip(norm, decode):\n",
    "                x1=d(n(x1))\n",
    "                x1=self.decode_act(x1)\n",
    "            x_out.append(x1)\n",
    "        x_out=cat(x_out, dim=1)\n",
    "        \n",
    "        # variance\n",
    "        if self.variance:\n",
    "            sig=[]\n",
    "            for norm, decode in zip(self.sig_norms, self.sigs):\n",
    "                x1=clone(x)\n",
    "                for n, d in zip(norm, decode):\n",
    "                    x1=d(n(x1))\n",
    "                    x1=self.decode_act(x1)\n",
    "                sig.append(x1)\n",
    "            sig=abs(cat(sig, dim=1))\n",
    "\n",
    "        if self.rho!=0:\n",
    "            rho=[]\n",
    "            for norm, decode in zip(self.rho_norms, self.rhos):\n",
    "                x1=clone(x)\n",
    "                for n, d in zip(norm, decode):\n",
    "                    x1=d(n(x1))\n",
    "                    x1=self.decode_act(x1)\n",
    "                rho.append(x1)\n",
    "            rho=abs(cat(rho, dim=1)) ### not sure this works with only 1d\n",
    "        \n",
    "        if self.variance:\n",
    "            if self.rho!=0:\n",
    "                return x_out, sig, tanh(rho)\n",
    "            else:\n",
    "                return x_out, sig\n",
    "        else:\n",
    "            return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8536c56",
   "metadata": {},
   "source": [
    "## Loss implementation\n",
    "\n",
    "I'll first try the 2d version formulated in the below fashion since $\\rho$ fits nicely with a sigmoid\n",
    "\n",
    "$$\n",
    "f(x, y)=\\frac{1}{2 \\pi \\sigma_{X} \\sigma_{Y} \\sqrt{1-\\rho^{2}}} \\exp \\left(-\\frac{1}{2\\left(1-\\rho^{2}\\right)}\\left[\\left(\\frac{x-\\mu_{X}}{\\sigma_{X}}\\right)^{2}-2 \\rho\\left(\\frac{x-\\mu_{X}}{\\sigma_{X}}\\right)\\left(\\frac{y-\\mu_{Y}}{\\sigma_{Y}}\\right)+\\left(\\frac{y-\\mu_{Y}}{\\sigma_{Y}}\\right)^{2}\\right]\\right)\n",
    "$$\n",
    "Where $\\rho$ is the correlation between $X$ and $Y$ and where $\\sigma_{X}>0$ and $\\sigma_{Y}>0 .$ In this case,\n",
    "$$\n",
    "\\boldsymbol{\\mu}=\\left(\\begin{array}{l}\n",
    "\\mu_{X} \\\\\n",
    "\\mu_{Y}\n",
    "\\end{array}\\right), \\quad \\boldsymbol{\\Sigma}=\\left(\\begin{array}{cc}\n",
    "\\sigma_{X}^{2} & \\rho \\sigma_{X} \\sigma_{Y} \\\\\n",
    "\\rho \\sigma_{X} \\sigma_{Y} & \\sigma_{Y}^{2}\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "\n",
    "Defining $$z_x = \\left(\\frac{x-\\mu_{X}}{\\sigma_{X}}\\right)$$, we can write the NLL as:\n",
    "\n",
    "$$\n",
    "ln(\\sigma_{x})+ln(\\sigma_{y})+1/2*ln(1-\\rho^2)+\\frac{1}{2(1-\\rho^2)}(z_x^2+z_y^2-2*\\rho*(z_x*z_y))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc962ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_func_2d(pred, ys, sig1, sig2, rho):\n",
    "#     global z1, z2, sigloss, rholoss, factor\n",
    "    z1=(pred[:,0]-ys[:,0])/sig1\n",
    "    z2=(pred[:,1]-ys[:,1])/sig2\n",
    "    sigloss=torch.sum(torch.log(sig1)+torch.log(sig2))\n",
    "    rholoss=torch.sum(torch.log(1-rho**2)/2)\n",
    "    factor=1/(2*(1-rho**2))\n",
    "    err_loss = torch.sum(factor*(z1**2+z2**2-2*rho*z1*z2))\n",
    "    \n",
    "    return err_loss+sigloss+rholoss, err_loss, sigloss, rholoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bef65c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_func_2d(pred, ys, sig, rho):\n",
    "#     global delta, bsize, A2, sig_inv, detloss, err_loss\n",
    "    \n",
    "    delta=pred-ys\n",
    "    bsize=delta.shape[0]\n",
    "    N=4\n",
    "    #this is messy but it works \n",
    "    \n",
    "    #compute the covariance matrix\n",
    "    vals = torch.vstack([sig[:,0]**2, rho[:,0]*sig[:,0]*sig[:,1], rho[:,1]*sig[:,0]*sig[:,2], rho[:,2]*sig[:,0]*sig[:,3],\\\n",
    "                 sig[:,1]**2,rho[:,3]*sig[:,1]*sig[:,2], rho[:,4]*sig[:,1]*sig[:,3], \\\n",
    "                sig[:,2]**2,rho[:,5]*sig[:,2]*sig[:,3],\\\n",
    "                 sig[:,3]**2])\n",
    "\n",
    "    A = torch.zeros(N, N,bsize, device='cuda:0')\n",
    "    A[0] = vals[:4]\n",
    "    A[1] = torch.vstack([vals[1], vals[4:7]])\n",
    "    A[2] = torch.vstack([vals[2], vals[5], vals[7:9]])\n",
    "    A[3] = torch.vstack([vals[3], vals[6], vals[8], vals[9]])\n",
    "    \n",
    "    A2=A.permute(2,0,1)\n",
    "    \n",
    "    det=torch.det(A2)\n",
    "    detloss=torch.sum(torch.log(det))/2\n",
    "    \n",
    "    sig_inv = torch.inverse(A2)\n",
    "    \n",
    "    err=delta*torch.bmm(sig_inv, delta.unsqueeze(2))[:,:,0]\n",
    "    \n",
    "    err_loss = torch.sum(err)/2\n",
    "    \n",
    "    return err_loss+detloss, err_loss, detloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86c2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "n_epochs=200\n",
    "n_trials=1\n",
    "batch_size=128\n",
    "split=0.8\n",
    "test_data=data[int(len(data)*split):]\n",
    "train_data=data[:int(len(data)*split)]\n",
    "l1_lambda = 0\n",
    "l2_lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88b67657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our train function\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    global out, sigs, rho\n",
    "    for data in train_loader:  \n",
    "        out, sigs, rho = model(data)  \n",
    "        loss, err_l, det_l = l_func_2d(out, data.y.view(-1,n_targ), sigs, rho) \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "\n",
    "        loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "#             loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "#     print(loss, l1_norm*l1_lambda, l2_norm*l2_lambda)\n",
    " # test function\n",
    "\n",
    "def test(loader): ##### transform back missing\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    ys = []\n",
    "    varss = []\n",
    "    rhos = []\n",
    "    with torch.no_grad(): ##this solves it!!!\n",
    "        for dat in loader: \n",
    "            out, sigs , rho  = model(dat) \n",
    "            ys.append(dat.y.view(-1,n_targ))\n",
    "            outs.append(out)\n",
    "            varss.append(sigs)\n",
    "            rhos.append(rho)\n",
    "    outss=torch.vstack(outs)\n",
    "    yss=torch.vstack(ys)\n",
    "    varss=torch.vstack(varss)\n",
    "    rhos=torch.vstack(rhos)\n",
    "    return torch.std(outss - yss, axis=0), outss, yss, varss, rhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13dde5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RelU conv activation\n",
      "LeakyRelU decode activation\n",
      "GPU  True\n"
     ]
    }
   ],
   "source": [
    "trains, tests, scatter = [], [], []\n",
    "yss, preds=[],[]\n",
    "model = Sage()\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=1, num_workers=1)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=0,num_workers=1)    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "_, _, test_loader = accelerator.prepare(model, optimizer, test_loader)\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "print('GPU ', next(model.parameters()).is_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a3a0e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train scatter: [0.935  0.1753 0.5431 0.9949] \n",
      "           Test scatter:  [0.9295 0.1734 0.5395 0.9948]\n",
      "Epoch: 004, Train scatter: [0.935  0.3712 0.5431 0.9949] \n",
      "           Test scatter:  [0.9295 0.3662 0.5395 0.9948]\n",
      "Epoch: 006, Train scatter: [0.9349 0.2094 0.5431 0.9949] \n",
      "           Test scatter:  [0.9295 0.2069 0.5395 0.9948]\n",
      "Epoch: 008, Train scatter: [0.9349 0.1872 0.5431 0.9949] \n",
      "           Test scatter:  [0.9295 0.185  0.5395 0.9948]\n",
      "Epoch: 010, Train scatter: [0.9349 0.2074 0.5431 0.9949] \n",
      "           Test scatter:  [0.9295 0.2042 0.5395 0.9947]\n",
      "Epoch: 012, Train scatter: [0.9349 0.1764 0.5431 0.9949] \n",
      "           Test scatter:  [0.9294 0.1744 0.5394 0.9948]\n",
      "Epoch: 014, Train scatter: [0.9348 0.172  0.5431 0.9948] \n",
      "           Test scatter:  [0.9294 0.1699 0.5394 0.9947]\n",
      "Epoch: 016, Train scatter: [0.9349 0.1609 0.5429 0.9949] \n",
      "           Test scatter:  [0.9294 0.1592 0.5393 0.9948]\n",
      "Epoch: 018, Train scatter: [0.9344 0.1374 0.5422 0.9949] \n",
      "           Test scatter:  [0.929  0.136  0.5386 0.9948]\n",
      "Epoch: 020, Train scatter: [0.92   0.1395 0.5195 0.9946] \n",
      "           Test scatter:  [0.9148 0.1369 0.5167 0.9945]\n",
      "Epoch: 022, Train scatter: [0.8915 0.1283 0.487  0.994 ] \n",
      "           Test scatter:  [0.8868 0.1253 0.4851 0.9939]\n",
      "Epoch: 024, Train scatter: [0.8077 0.096  0.3792 0.9624] \n",
      "           Test scatter:  [0.8066 0.0961 0.3963 0.9657]\n",
      "Epoch: 026, Train scatter: [0.5849 0.0828 0.2923 0.6664] \n",
      "           Test scatter:  [0.5821 0.0831 0.3038 0.6694]\n",
      "Epoch: 028, Train scatter: [0.5423 0.0654 0.2585 0.6013] \n",
      "           Test scatter:  [0.539  0.0653 0.2669 0.6046]\n",
      "Epoch: 030, Train scatter: [0.4824 0.0586 0.2383 0.5555] \n",
      "           Test scatter:  [0.4775 0.0587 0.2456 0.5588]\n",
      "Epoch: 032, Train scatter: [0.3347 0.0516 0.2334 0.5342] \n",
      "           Test scatter:  [0.3333 0.0514 0.2398 0.5384]\n",
      "Epoch: 034, Train scatter: [0.2282 0.0499 0.2241 0.5103] \n",
      "           Test scatter:  [0.2258 0.0489 0.2289 0.5135]\n",
      "Epoch: 036, Train scatter: [0.218  0.0575 0.2785 0.5306] \n",
      "           Test scatter:  [0.2141 0.0572 0.2806 0.5324]\n",
      "Epoch: 038, Train scatter: [0.1591 0.0441 0.2216 0.4982] \n",
      "           Test scatter:  [0.1543 0.0435 0.2267 0.4994]\n",
      "Epoch: 040, Train scatter: [0.1476 0.0426 0.2181 0.4764] \n",
      "           Test scatter:  [0.1407 0.042  0.2219 0.4735]\n",
      "Epoch: 042, Train scatter: [0.1404 0.0418 0.2145 0.4383] \n",
      "           Test scatter:  [0.1328 0.0413 0.2184 0.4347]\n",
      "Epoch: 044, Train scatter: [0.765  0.1779 0.543  0.9939] \n",
      "           Test scatter:  [0.7627 0.1759 0.5393 0.9938]\n",
      "Epoch: 046, Train scatter: [0.718  0.1624 0.5431 0.9856] \n",
      "           Test scatter:  [0.7164 0.1607 0.5394 0.9856]\n",
      "Epoch: 048, Train scatter: [0.6043 0.1216 0.5425 0.7774] \n",
      "           Test scatter:  [0.603  0.1204 0.5389 0.7795]\n",
      "Epoch: 050, Train scatter: [0.5844 0.118  0.4989 0.7623] \n",
      "           Test scatter:  [0.5851 0.1169 0.4968 0.7639]\n",
      "Epoch: 052, Train scatter: [0.5205 0.1163 0.4924 0.7517] \n",
      "           Test scatter:  [0.5214 0.115  0.4901 0.7521]\n",
      "Epoch: 054, Train scatter: [0.5059 0.1141 0.491  0.7451] \n",
      "           Test scatter:  [0.5062 0.1128 0.489  0.7451]\n",
      "Epoch: 056, Train scatter: [0.4815 0.1068 0.4784 0.7261] \n",
      "           Test scatter:  [0.482  0.1056 0.4758 0.7239]\n",
      "Epoch: 058, Train scatter: [0.3986 0.1001 0.4624 0.677 ] \n",
      "           Test scatter:  [0.3949 0.0986 0.459  0.6706]\n",
      "Epoch: 060, Train scatter: [0.7076 0.1386 0.5086 0.9613] \n",
      "           Test scatter:  [0.6987 0.1364 0.5059 0.961 ]\n",
      "Epoch: 062, Train scatter: [0.2726 0.0875 0.4296 0.6042] \n",
      "           Test scatter:  [0.2702 0.0862 0.4285 0.5999]\n",
      "Epoch: 064, Train scatter: [0.2316 0.0756 0.3988 0.562 ] \n",
      "           Test scatter:  [0.2307 0.0743 0.3974 0.5576]\n",
      "Epoch: 066, Train scatter: [0.2015 0.0722 0.3934 0.5503] \n",
      "           Test scatter:  [0.1982 0.0711 0.3921 0.5465]\n",
      "Epoch: 068, Train scatter: [0.1812 0.0677 0.3768 0.5224] \n",
      "           Test scatter:  [0.1761 0.0665 0.3754 0.5174]\n",
      "Epoch: 070, Train scatter: [0.1614 0.0616 0.3568 0.5097] \n",
      "           Test scatter:  [0.1595 0.0607 0.3562 0.5051]\n",
      "Epoch: 072, Train scatter: [0.5263 0.1198 0.5129 0.7196] \n",
      "           Test scatter:  [0.5252 0.1184 0.5102 0.7199]\n",
      "Epoch: 074, Train scatter: [0.4916 0.1216 0.5039 0.7061] \n",
      "           Test scatter:  [0.4909 0.1202 0.501  0.7049]\n",
      "Epoch: 076, Train scatter: [0.4192 0.1191 0.5019 0.6885] \n",
      "           Test scatter:  [0.4165 0.1171 0.4982 0.6861]\n",
      "Epoch: 078, Train scatter: [0.354  0.1008 0.4422 0.615 ] \n",
      "           Test scatter:  [0.3524 0.099  0.4385 0.6133]\n",
      "Epoch: 080, Train scatter: [0.2258 0.0824 0.3943 0.5478] \n",
      "           Test scatter:  [0.2259 0.0815 0.3938 0.5495]\n",
      "Epoch: 082, Train scatter: [0.1812 0.0673 0.3632 0.505 ] \n",
      "           Test scatter:  [0.1742 0.0661 0.3626 0.5044]\n",
      "Epoch: 084, Train scatter: [0.1683 0.0571 0.3118 0.4937] \n",
      "           Test scatter:  [0.1625 0.0559 0.3123 0.4929]\n",
      "Epoch: 086, Train scatter: [0.1566 0.0484 0.26   0.445 ] \n",
      "           Test scatter:  [0.1479 0.0473 0.2621 0.4427]\n",
      "Epoch: 088, Train scatter: [0.1417 0.0455 0.2372 0.4309] \n",
      "           Test scatter:  [0.1371 0.0449 0.2393 0.4271]\n",
      "Epoch: 090, Train scatter: [0.1364 0.0436 0.2306 0.4208] \n",
      "           Test scatter:  [0.1336 0.0429 0.2326 0.4173]\n",
      "Epoch: 092, Train scatter: [0.1326 0.0429 0.2247 0.4188] \n",
      "           Test scatter:  [0.1348 0.0423 0.228  0.4148]\n",
      "Epoch: 094, Train scatter: [0.1264 0.0429 0.219  0.4116] \n",
      "           Test scatter:  [0.1268 0.0425 0.2222 0.4079]\n",
      "Epoch: 096, Train scatter: [0.1302 0.0414 0.2168 0.413 ] \n",
      "           Test scatter:  [0.1274 0.0407 0.2195 0.4065]\n",
      "Epoch: 098, Train scatter: [0.1291 0.0428 0.2192 0.4113] \n",
      "           Test scatter:  [0.1277 0.0422 0.2216 0.4065]\n",
      "Epoch: 100, Train scatter: [0.3081 0.0952 0.5047 0.6316] \n",
      "           Test scatter:  [0.3002 0.0937 0.5016 0.6281]\n",
      "Epoch: 102, Train scatter: [0.1443 0.0531 0.2691 0.4479] \n",
      "           Test scatter:  [0.142  0.0526 0.2718 0.4477]\n",
      "Epoch: 104, Train scatter: [0.5057 0.1202 0.5009 0.7212] \n",
      "           Test scatter:  [0.5065 0.1196 0.5    0.7261]\n",
      "Epoch: 106, Train scatter: [0.1456 0.0532 0.2949 0.4673] \n",
      "           Test scatter:  [0.1469 0.0529 0.2979 0.467 ]\n",
      "Epoch: 108, Train scatter: [0.1301 0.0438 0.2279 0.4254] \n",
      "           Test scatter:  [0.1302 0.0434 0.2316 0.4232]\n",
      "Epoch: 110, Train scatter: [0.3643 0.0591 0.2663 0.4866] \n",
      "           Test scatter:  [0.3583 0.0577 0.2676 0.4852]\n",
      "Epoch: 112, Train scatter: [0.1242 0.0413 0.2177 0.4144] \n",
      "           Test scatter:  [0.123  0.0406 0.2211 0.41  ]\n",
      "Epoch: 114, Train scatter: [0.1233 0.0398 0.2131 0.4102] \n",
      "           Test scatter:  [0.1207 0.0391 0.2168 0.4038]\n",
      "Epoch: 116, Train scatter: [0.152  0.0738 0.5107 0.5481] \n",
      "           Test scatter:  [0.1533 0.0733 0.5105 0.5506]\n",
      "Epoch: 118, Train scatter: [0.1252 0.0409 0.2204 0.4236] \n",
      "           Test scatter:  [0.124  0.0403 0.2243 0.4203]\n",
      "Epoch: 120, Train scatter: [0.1198 0.0397 0.214  0.4078] \n",
      "           Test scatter:  [0.1193 0.039  0.2165 0.4019]\n",
      "Epoch: 122, Train scatter: [0.1176 0.0386 0.2118 0.4042] \n",
      "           Test scatter:  [0.1169 0.038  0.2139 0.3997]\n",
      "Epoch: 124, Train scatter: [0.1194 0.0385 0.2105 0.4053] \n",
      "           Test scatter:  [0.1182 0.0379 0.2131 0.3983]\n",
      "Epoch: 126, Train scatter: [0.1182 0.038  0.2102 0.4041] \n",
      "           Test scatter:  [0.118  0.0374 0.2122 0.3978]\n",
      "Epoch: 128, Train scatter: [0.1233 0.0384 0.213  0.407 ] \n",
      "           Test scatter:  [0.1247 0.0379 0.2169 0.4014]\n",
      "Epoch: 130, Train scatter: [0.1147 0.0377 0.2102 0.4007] \n",
      "           Test scatter:  [0.1144 0.0372 0.2122 0.3942]\n",
      "Epoch: 132, Train scatter: [0.1143 0.0371 0.2084 0.4002] \n",
      "           Test scatter:  [0.1138 0.0366 0.2112 0.3926]\n",
      "Epoch: 134, Train scatter: [0.1198 0.0395 0.2126 0.4105] \n",
      "           Test scatter:  [0.1195 0.039  0.2149 0.4049]\n",
      "Epoch: 136, Train scatter: [0.1151 0.0374 0.2112 0.3998] \n",
      "           Test scatter:  [0.1136 0.0369 0.2146 0.3923]\n",
      "Epoch: 138, Train scatter: [0.1146 0.0369 0.2067 0.395 ] \n",
      "           Test scatter:  [0.1132 0.0365 0.2097 0.3891]\n",
      "Epoch: 140, Train scatter: [0.1098 0.0366 0.2061 0.3934] \n",
      "           Test scatter:  [0.1111 0.0362 0.209  0.3885]\n",
      "Epoch: 142, Train scatter: [0.1104 0.0367 0.2048 0.3932] \n",
      "           Test scatter:  [0.1109 0.0362 0.208  0.3878]\n",
      "Epoch: 144, Train scatter: [0.109  0.0364 0.2051 0.392 ] \n",
      "           Test scatter:  [0.1106 0.0362 0.2084 0.3861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 146, Train scatter: [0.1066 0.0365 0.2043 0.3931] \n",
      "           Test scatter:  [0.108  0.0361 0.2076 0.3871]\n",
      "Epoch: 148, Train scatter: [0.1057 0.0363 0.2044 0.3924] \n",
      "           Test scatter:  [0.1065 0.0359 0.2081 0.3873]\n",
      "Epoch: 150, Train scatter: [0.1102 0.0369 0.2076 0.3984] \n",
      "           Test scatter:  [0.1107 0.0365 0.2111 0.392 ]\n",
      "Epoch: 152, Train scatter: [0.1081 0.0365 0.2034 0.3925] \n",
      "           Test scatter:  [0.1089 0.036  0.2071 0.3864]\n",
      "Epoch: 154, Train scatter: [0.1068 0.036  0.2022 0.3884] \n",
      "           Test scatter:  [0.1068 0.0357 0.2062 0.3831]\n",
      "Epoch: 156, Train scatter: [0.1049 0.0358 0.2022 0.3884] \n",
      "           Test scatter:  [0.1051 0.0355 0.2062 0.3828]\n",
      "Epoch: 158, Train scatter: [0.1089 0.036  0.203  0.389 ] \n",
      "           Test scatter:  [0.1094 0.0357 0.2068 0.3841]\n",
      "Epoch: 160, Train scatter: [0.1068 0.036  0.2013 0.3893] \n",
      "           Test scatter:  [0.1076 0.0356 0.2053 0.3838]\n",
      "Epoch: 162, Train scatter: [0.1041 0.0355 0.2007 0.3859] \n",
      "           Test scatter:  [0.1048 0.0352 0.2045 0.3799]\n",
      "Epoch: 164, Train scatter: [0.1053 0.0355 0.2008 0.3868] \n",
      "           Test scatter:  [0.1059 0.0352 0.2049 0.3821]\n",
      "Epoch: 166, Train scatter: [0.1052 0.0356 0.2001 0.386 ] \n",
      "           Test scatter:  [0.1055 0.0353 0.2042 0.3807]\n",
      "Epoch: 168, Train scatter: [0.635  0.1436 0.5431 0.8979] \n",
      "           Test scatter:  [0.6291 0.1424 0.5395 0.8996]\n",
      "Epoch: 170, Train scatter: [0.2274 0.0806 0.5436 0.6649] \n",
      "           Test scatter:  [0.2282 0.0793 0.5401 0.6661]\n",
      "Epoch: 172, Train scatter: [0.1706 0.0698 0.492  0.5706] \n",
      "           Test scatter:  [0.1706 0.0684 0.4879 0.5701]\n",
      "Epoch: 174, Train scatter: [0.1647 0.0618 0.4096 0.5155] \n",
      "           Test scatter:  [0.1645 0.0604 0.4065 0.5127]\n",
      "Epoch: 176, Train scatter: [0.1442 0.0528 0.3416 0.4671] \n",
      "           Test scatter:  [0.143  0.0517 0.3465 0.4633]\n",
      "Epoch: 178, Train scatter: [0.1378 0.0479 0.2657 0.4378] \n",
      "           Test scatter:  [0.135  0.0475 0.2673 0.4325]\n",
      "Epoch: 180, Train scatter: [0.1347 0.044  0.2368 0.4257] \n",
      "           Test scatter:  [0.1321 0.0436 0.2402 0.4195]\n",
      "Epoch: 182, Train scatter: [0.1346 0.0459 0.2538 0.4642] \n",
      "           Test scatter:  [0.1333 0.045  0.2541 0.4578]\n",
      "Epoch: 184, Train scatter: [0.1197 0.0415 0.23   0.4218] \n",
      "           Test scatter:  [0.1193 0.0411 0.2349 0.4154]\n",
      "Epoch: 186, Train scatter: [0.1238 0.0402 0.2264 0.4151] \n",
      "           Test scatter:  [0.1219 0.0398 0.2302 0.4081]\n",
      "Epoch: 188, Train scatter: [0.1175 0.039  0.2231 0.4107] \n",
      "           Test scatter:  [0.1174 0.0386 0.2269 0.4037]\n",
      "Epoch: 190, Train scatter: [0.1142 0.0383 0.2193 0.4057] \n",
      "           Test scatter:  [0.114  0.0379 0.2224 0.4003]\n",
      "Epoch: 192, Train scatter: [0.1166 0.0379 0.2136 0.4019] \n",
      "           Test scatter:  [0.1162 0.0377 0.217  0.3953]\n",
      "Epoch: 194, Train scatter: [0.1102 0.0371 0.2113 0.3972] \n",
      "           Test scatter:  [0.1086 0.0367 0.2148 0.3907]\n",
      "Epoch: 196, Train scatter: [0.1124 0.0365 0.2137 0.3981] \n",
      "           Test scatter:  [0.1102 0.0362 0.2163 0.3906]\n",
      "Epoch: 198, Train scatter: [0.8928 0.1558 0.543  0.9904] \n",
      "           Test scatter:  [0.8873 0.1541 0.5393 0.9903]\n",
      "Epoch: 200, Train scatter: [0.9362 0.1416 0.543  2.4577] \n",
      "           Test scatter:  [0.9307 0.1406 0.5394 2.3907]\n",
      "11802.88 seconds spent training, 59.014 seconds per epoch. Processed 1475 trees per second\n"
     ]
    }
   ],
   "source": [
    "#this uses about 1 GB of memory on the GPU\n",
    "tr_acc, te_acc = [], []\n",
    "start=time.time()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    train()\n",
    "\n",
    "    if (epoch+1)%2==0:\n",
    "        train_acc, _ , _, _ , _ = test(train_loader)\n",
    "        test_acc, _ , _ , _ , _ = test(test_loader)\n",
    "        tr_acc.append(train_acc.cpu().numpy())\n",
    "        te_acc.append(test_acc.cpu().numpy())\n",
    "        print(f'Epoch: {epoch+1:03d}, Train scatter: {np.round(train_acc.cpu().numpy(), 4)} \\n \\\n",
    "          Test scatter:  {np.round(test_acc.cpu().numpy(), 4)}')\n",
    "stop=time.time()\n",
    "spent=stop-start\n",
    "print(f\"{spent:.2f} seconds spent training, {spent/n_epochs:.3f} seconds per epoch. Processed {len(data)*split*n_epochs/spent:.0f} trees per second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd971c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=120\n",
    "te_acc, tr_acc = np.array(te_acc), np.array(tr_acc)\n",
    "fig, ax =plt.subplots(2,2,figsize=(15,7))\n",
    "ax=ax.flatten()\n",
    "best = [0.0781, 0.034,0.1834, 0.3608]\n",
    "for i in range(4):\n",
    "    ax[i].plot(np.arange(0,n_epochs,2), tr_acc[:,i], 'r-', label=f'training')\n",
    "    ax[i].plot(np.arange(0,n_epochs,2), te_acc[:,i], 'g-',label=f'validation')\n",
    "    ax[i].hlines(best[i], 0, 120, color='k', linestyle='dashed', label='best in single training')\n",
    "    ax[i].set(xlabel='Epoch', ylabel=r'$\\sigma$ stellar mass [dex]', \\\n",
    "        title=f'{cols_t[i][:6]} Minimum {np.min(np.array(te_acc)[:,i]):.3f}, median last 10 {np.median(np.array(te_acc)[-5:,i]):.3f}')\n",
    "    ax[i].legend()\n",
    "\n",
    "# ax[1].plot(np.arange(0,n_epochs,2), tr_acc[:,1], 'r-', label='training')\n",
    "# ax[1].plot(np.arange(0,n_epochs,2), te_acc[:,1], 'g-',label='validation')\n",
    "# ax[1].set(xlabel='Epoch', ylabel=r'$\\sigma$ v_disk [dex]', \\\n",
    "#     title=f'Minimum {np.min(np.array(te_acc)[:,1]):.4f}, median last 10 {np.median(np.array(te_acc)[-5:,1]):.4f}')\n",
    "# ax[1].legend()\n",
    "\n",
    "fig.suptitle('4D Gaussian Loss with correlation')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a6fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(te_acc)[-5:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b91922",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(te_acc)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49038ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainstd, outtrain, ytrain, var, rho = test(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(2, figsize=(15,8))\n",
    "ax = ax.flatten()\n",
    "l=0\n",
    "for k in range(n_targ):\n",
    "#     ax[k].hist(ress.cpu().numpy()[:,k], bins=50, histtype='step', label='res')\n",
    "    ax[k].hist(outtrain.cpu().numpy()[:,k], bins=50, \\\n",
    "            range=list(np.percentile(outtrain.cpu().numpy()[:,k], [l,100-l])), histtype='step', label='pred')\n",
    "    ax[k].hist(ytrain.cpu().numpy()[:,k], bins=50, \\\n",
    "            range=list(np.percentile(outtrain.cpu().numpy()[:,k], [l,100-l])), histtype='step', label='true')\n",
    "\n",
    "    ax[k].legend()\n",
    "#     print(np.std(ress.cpu().numpy()[:,k]), np.mean(ress.cpu().numpy()[:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rho.cpu().numpy().flatten(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e097dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "plt.hist((outtrain.cpu().numpy()[:,i]-ytrain.cpu().numpy()[:,i])/var[:,i].cpu().numpy(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b730bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(var.cpu().numpy()[:,0], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "i=0\n",
    "z0=(outtrain.cpu().numpy()[:,i]-ytrain.cpu().numpy()[:,i])/var[:,i].cpu().numpy()\n",
    "i=1\n",
    "z1=(outtrain.cpu().numpy()[:,i]-ytrain.cpu().numpy()[:,i])/var[:,i].cpu().numpy()\n",
    "vals, x, y, ax=plt.hist2d(z0,z1,bins=25, norm=mpl.colors.LogNorm(), cmap=mpl.cm.magma)\n",
    "X, Y = np.meshgrid((x[1:]+x[:-1])/2, (y[1:]+y[:-1])/2)\n",
    "plt.contour(X,Y, np.log(vals.T+1), levels=10, colors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(z0,z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "vals, x, y, ax=plt.hist2d(outtrain.cpu().numpy()[:,0],outtrain.cpu().numpy()[:,1],bins=25, norm=mpl.colors.LogNorm(), cmap=mpl.cm.magma)\n",
    "X, Y = np.meshgrid((x[1:]+x[:-1])/2, (y[1:]+y[:-1])/2)\n",
    "plt.contour(X,Y, np.log(vals.T+1), levels=10, colors='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(outtrain.cpu().numpy()[:,0],outtrain.cpu().numpy()[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x[1:]+x[:-1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(2, figsize=(8,10))\n",
    "ax = ax.flatten()\n",
    "l=0.5\n",
    "for k in range(n_targ):\n",
    "    ytr=ytrain.cpu().numpy()[:,k]\n",
    "    predtr=outtrain.cpu().numpy()[:,k]\n",
    "    ax[k].plot(ytr, predtr, 'ro', label='true-pred')\n",
    "    ax[k].plot([min(ytr),max(ytr)],[min(ytr),max(ytr)], 'k--', label='Perfect correspondance')\n",
    "    ax[k].set(title=[cols_t[k], np.round(np.std(ytr-predtr),3), np.round(np.mean(ytr-predtr),3)])\n",
    "    ax[k].legend()\n",
    "#     print(np.std(ress.cpu().numpy()[:,k]), np.mean(ress.cpu().numpy()[:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "teststd, outtest, ytest, vtest, rhotest = test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(2, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "l=0.0\n",
    "for k in range(n_targ):\n",
    "    ax[k].hist(outtest.cpu().numpy()[:,k], bins=50, \\\n",
    "            range=list(np.percentile(outtest.cpu().numpy()[:,k], [l,100-l])), histtype='step', label='pred')\n",
    "    ax[k].hist(ytest.cpu().numpy()[:,k], bins=50, \\\n",
    "            range=list(np.percentile(outtest.cpu().numpy()[:,k], [l,100-l])), histtype='step', label='true')\n",
    "    ax[k].set(title=cols_t[k])\n",
    "    ax[k].legend()\n",
    "#     print(np.std(ress.cpu().numpy()[:,k]), np.mean(ress.cpu().numpy()[:,k]))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2539ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig , ax = plt.subplots(2, figsize=(7,10))\n",
    "# ax = ax.flatten()\n",
    "# for k in range(n_targ):\n",
    "#     yte=ytest.cpu().numpy()[:,k]\n",
    "#     predte=outtest.cpu().numpy()[:,k]\n",
    "#     ax[k].plot(yte, predte, 'ro', label='true-pred')\n",
    "#     ax[k].plot([min(yte),max(yte)],[min(yte),max(yte)], 'k--', label='Perfect correspondance')\n",
    "#     ax[k].set(title=[cols_t[k], np.round(np.std(yte-predte),3), np.round(np.mean(yte-predte),3)])\n",
    "#     ax[k].legend()\n",
    "# #     print(np.std(ress.cpu().numpy()[:,k]), np.mean(ress.cpu().numpy()[:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993edf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75054468",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=torch.rand(128,4)\n",
    "pred=torch.rand(128,4)\n",
    "sig = torch.abs(torch.rand(128,4))\n",
    "rho = torch.tanh(torch.rand(128,6)-torch.ones(128,6)/2)\n",
    "# ((pred-ys)/sig)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d448538",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=ys-pred\n",
    "d.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12466c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = torch.vstack([sig[:,0]**2, rho[:,0]*sig[:,0]*sig[:,1], rho[:,1]*sig[:,0]*sig[:,2], rho[:,2]*sig[:,0]*sig[:,3],\\\n",
    "                 sig[:,1]**2,rho[:,3]*sig[:,1]*sig[:,2], rho[:,4]*sig[:,1]*sig[:,3], \\\n",
    "                sig[:,2]**2,rho[:,5]*sig[:,2]*sig[:,3],\\\n",
    "                 sig[:,3]**2])\n",
    "\n",
    "A = torch.zeros(N, N,128)\n",
    "\n",
    "A[0] = vals[:4]\n",
    "A[1] = torch.vstack([vals[1], vals[4:7]])\n",
    "A[2] = torch.vstack([vals[2], vals[5], vals[7:9]])\n",
    "A[3] = torch.vstack([vals[3], vals[6], vals[8], vals[9]])\n",
    "\n",
    "A2=A.permute(2,0,1)\n",
    "det=torch.det(A2)\n",
    "\n",
    "sig_inv = torch.inverse(A2)\n",
    "\n",
    "d*torch.bmm(sig_inv, d.unsqueeze(2))[:,:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
