{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e649e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "import torch, pickle, time, os, random\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.loader import DataLoader\n",
    "# accelerate huggingface to GPU\n",
    "if torch.cuda.is_available():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "from pysr import pysr, best\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "print('Loading data')\n",
    "\n",
    "case='vlarge_all_4t_z0.0_standard_raw'\n",
    "\n",
    "datat=pickle.load(open(osp.expanduser(f'~/../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed78ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "data=[]\n",
    "\n",
    "for d in datat:\n",
    "#     data.append(Data(x=d.x[:,[0,3,4,19,20]], edge_index=d.edge_index, edge_attr=d.edge_attr, y=d.y[0]))\n",
    "    data.append(Data(x=d.x, edge_index=d.edge_index, edge_attr=d.edge_attr, y=d.y[0]))\n",
    "    \n",
    "\n",
    "try:\n",
    "    n_targ=len(data[0].y)\n",
    "except:\n",
    "    n_targ=1\n",
    "n_feat=len(data[0].x[0])\n",
    "n_feat, n_targ\n",
    "\n",
    "print('Loaded data')\n",
    "\n",
    "from torch.nn import ReLU, Linear, Module, LayerNorm, Sequential\n",
    "class MLP(Module):\n",
    "    def __init__(self, n_in, n_out, hidden=128, nlayers=4, layer_norm=True):\n",
    "        super().__init__()\n",
    "        layers = [Linear(n_in, hidden), ReLU()]\n",
    "        for i in range(nlayers):\n",
    "            layers.append(Linear(hidden, hidden))\n",
    "            layers.append(ReLU()) \n",
    "        if layer_norm:\n",
    "            layers.append(LayerNorm(hidden)) #yay\n",
    "        layers.append(Linear(hidden, n_out))\n",
    "        self.mlp = Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5d70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageMLP(torch.nn.Module):\n",
    "    def __init__(self, n_in = 10, n_outs=10, hidden_channels=64):\n",
    "        super(SageMLP, self).__init__()\n",
    "        self.node = MLP(n_in, n_outs,  hidden = hidden_channels)\n",
    "        self.edge = MLP(n_in, n_outs, hidden = hidden_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        adj = edge_index # zero is sender, 1 is receiver\n",
    "        \n",
    "        neighborhood = scatter_add(x[adj[0]],adj[1], dim=0) #this adds only the neighbors, not the node itself\n",
    "        xe = self.edge( neighborhood ) #ENCODE EDGE SUM\n",
    "        x = self.node(x)\n",
    "        x0 = torch.clone(x)\n",
    "        \n",
    "        x[adj[1]]+= xe[adj[1]] ##edge added node\n",
    "        ## maybe add a joint decoder here\n",
    "        return x, x0, xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461c14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_scatter import scatter_add, scatter_mean\n",
    "\n",
    "class PySRNet(torch.nn.Module):\n",
    "    def __init__(self, n_m=10, hidden_channels=64, n_feat=43, n_targ=1):\n",
    "        super(PySRNet, self).__init__()\n",
    "        self.n0 = MLP(n_feat, n_m, hidden = hidden_channels)\n",
    "        self.conv = SageMLP( n_m, n_m, hidden_channels = hidden_channels) \n",
    "    \n",
    "        self.decode = MLP(n_m, n_targ,  hidden = hidden_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        \n",
    "        x = self.n0(x) # NODE ENCODER\n",
    "        x0 = torch.clone(x)\n",
    "        \n",
    "        x, xn, xe = self.conv(x, edge_index)\n",
    "        x1 = torch.clone(x)\n",
    "        \n",
    "        x = global_add_pool(x, batch)\n",
    "\n",
    "        x = self.decode(x)\n",
    "\n",
    "        return x, x0, x1, xn, xe\n",
    "    \n",
    "model = PySRNet(hidden_channels=128)\n",
    "next(model.parameters()).is_cuda ##check number one\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.L1Loss()\n",
    "# criterion = torch.nn.SmoothL1Loss(beta=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030cb3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU  False\n"
     ]
    }
   ],
   "source": [
    "# data=data\n",
    "n_epochs=50\n",
    "n_trials=1\n",
    "batch_size=int(2**8) # 8 = 256\n",
    "split=0.8\n",
    "test_data=data[int(len(data)*split):]\n",
    "train_data=data[:int(len(data)*split)]\n",
    "# train_data, test_data=train_test_split(data, test_size=0.2)\n",
    "l1_lambda = 1e-4\n",
    "l2_lambda = 0\n",
    "\n",
    "l1_n0 = 1\n",
    "l1_node = 1\n",
    "l1_edge = 1\n",
    "hidden = 128\n",
    "\n",
    "yss, preds=[],[]\n",
    "model = PySRNet(hidden_channels=hidden)\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=1, num_workers=4)\n",
    "\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=0, num_workers=4)    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "print('GPU ', next(model.parameters()).is_cuda)\n",
    "# Initialize our train function\n",
    "\n",
    "def train(k):\n",
    "    model.train()\n",
    "    global latent0, latent1, latn, latn\n",
    "    for data in tqdm(train_loader, total=len(train_loader)): \n",
    "#         print('batch')\n",
    "        out, latent0, latent1, latn, late = model(data.x, data.edge_index, data.batch)  \n",
    "        latent0_l, latent1_l, latn_l, late_l = latent0[:,2:], latent1[:,2:], latn[:,2:], late[:,2:]\n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        \n",
    "        n0_latent = torch.mean(torch.abs(latent0_l))\n",
    "        node_latent = torch.mean(torch.abs(latn_l))\n",
    "        edge_latent = torch.mean(torch.abs(late_l))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "#         l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "        if k%50==0:\n",
    "            print(loss, l1_n0 * n0_latent, l1_node * node_latent, l1_edge * edge_latent)\n",
    "        \n",
    "        k+=1\n",
    "        loss = loss + l1_n0 * n0_latent + l1_node * node_latent + l1_edge * edge_latent\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "#     print(loss, l1_norm*l1_lambda, l2_norm*l2_lambda)\n",
    " # test function\n",
    "\n",
    "def test(loader): ##### transform back missing\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    ys = []\n",
    "    with torch.no_grad(): ##this solves it!!!\n",
    "        for dat in tqdm(loader, total=len(loader)): \n",
    "            \n",
    "            out, latent0, latent1, latn, late = model(dat.x, dat.edge_index, dat.batch) \n",
    "            ys.append(dat.y.view(-1,n_targ))\n",
    "            outs.append(out)\n",
    "    outss=torch.vstack(outs)\n",
    "    yss=torch.vstack(ys)\n",
    "    return torch.std(outss - yss, axis=0), outss, yss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f543fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w_mask(k):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf75f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6412, grad_fn=<MseLossBackward>) tensor(0.3737, grad_fn=<MulBackward0>) tensor(0.8578, grad_fn=<MulBackward0>) tensor(0.4623, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [01:00<04:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7926, grad_fn=<MseLossBackward>) tensor(0.0473, grad_fn=<MulBackward0>) tensor(0.0501, grad_fn=<MulBackward0>) tensor(0.0312, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:51<03:53,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8269, grad_fn=<MseLossBackward>) tensor(0.0489, grad_fn=<MulBackward0>) tensor(0.0268, grad_fn=<MulBackward0>) tensor(0.0307, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:44<03:41,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8995, grad_fn=<MseLossBackward>) tensor(0.0364, grad_fn=<MulBackward0>) tensor(0.1480, grad_fn=<MulBackward0>) tensor(0.1354, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:34<02:51,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7794, grad_fn=<MseLossBackward>) tensor(0.0302, grad_fn=<MulBackward0>) tensor(0.0320, grad_fn=<MulBackward0>) tensor(0.0256, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:26<01:43,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2473, grad_fn=<MseLossBackward>) tensor(0.0296, grad_fn=<MulBackward0>) tensor(0.1030, grad_fn=<MulBackward0>) tensor(0.0793, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:25<00:42,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1166, grad_fn=<MseLossBackward>) tensor(0.0385, grad_fn=<MulBackward0>) tensor(0.0134, grad_fn=<MulBackward0>) tensor(0.0171, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [06:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2519, grad_fn=<MseLossBackward>) tensor(0.0465, grad_fn=<MulBackward0>) tensor(0.0159, grad_fn=<MulBackward0>) tensor(0.0173, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:56<05:25,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0976, grad_fn=<MseLossBackward>) tensor(0.0316, grad_fn=<MulBackward0>) tensor(0.0136, grad_fn=<MulBackward0>) tensor(0.0129, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:47<05:15,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1150, grad_fn=<MseLossBackward>) tensor(0.0401, grad_fn=<MulBackward0>) tensor(0.0109, grad_fn=<MulBackward0>) tensor(0.0107, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:42<03:46,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0982, grad_fn=<MseLossBackward>) tensor(0.0281, grad_fn=<MulBackward0>) tensor(0.0146, grad_fn=<MulBackward0>) tensor(0.0124, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:41<03:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1015, grad_fn=<MseLossBackward>) tensor(0.0293, grad_fn=<MulBackward0>) tensor(0.0106, grad_fn=<MulBackward0>) tensor(0.0092, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:38<02:02,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1200, grad_fn=<MseLossBackward>) tensor(0.0273, grad_fn=<MulBackward0>) tensor(0.0123, grad_fn=<MulBackward0>) tensor(0.0167, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:24<00:44,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0926, grad_fn=<MseLossBackward>) tensor(0.0219, grad_fn=<MulBackward0>) tensor(0.0124, grad_fn=<MulBackward0>) tensor(0.0105, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [06:06<00:00,  1.07s/it]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [02:29<00:00,  2.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:36<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Train scatter: [0.3331] \n",
      "         Test scatter: [0.332]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1490, grad_fn=<MseLossBackward>) tensor(0.0246, grad_fn=<MulBackward0>) tensor(0.0166, grad_fn=<MulBackward0>) tensor(0.0084, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:59<06:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0999, grad_fn=<MseLossBackward>) tensor(0.0215, grad_fn=<MulBackward0>) tensor(0.0129, grad_fn=<MulBackward0>) tensor(0.0148, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:51<04:29,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1132, grad_fn=<MseLossBackward>) tensor(0.0240, grad_fn=<MulBackward0>) tensor(0.0088, grad_fn=<MulBackward0>) tensor(0.0097, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:45<02:23,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1047, grad_fn=<MseLossBackward>) tensor(0.0227, grad_fn=<MulBackward0>) tensor(0.0127, grad_fn=<MulBackward0>) tensor(0.0147, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:37<01:43,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0953, grad_fn=<MseLossBackward>) tensor(0.0181, grad_fn=<MulBackward0>) tensor(0.0096, grad_fn=<MulBackward0>) tensor(0.0140, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:24<01:30,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1916, grad_fn=<MseLossBackward>) tensor(0.0180, grad_fn=<MulBackward0>) tensor(0.0083, grad_fn=<MulBackward0>) tensor(0.0108, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:10<00:45,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1130, grad_fn=<MseLossBackward>) tensor(0.0140, grad_fn=<MulBackward0>) tensor(0.0138, grad_fn=<MulBackward0>) tensor(0.0137, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:45<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0837, grad_fn=<MseLossBackward>) tensor(0.0086, grad_fn=<MulBackward0>) tensor(0.0094, grad_fn=<MulBackward0>) tensor(0.0143, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:53<04:39,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1121, grad_fn=<MseLossBackward>) tensor(0.0128, grad_fn=<MulBackward0>) tensor(0.0132, grad_fn=<MulBackward0>) tensor(0.0099, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:46<05:28,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0980, grad_fn=<MseLossBackward>) tensor(0.0144, grad_fn=<MulBackward0>) tensor(0.0100, grad_fn=<MulBackward0>) tensor(0.0102, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:36<03:35,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1104, grad_fn=<MseLossBackward>) tensor(0.0128, grad_fn=<MulBackward0>) tensor(0.0103, grad_fn=<MulBackward0>) tensor(0.0125, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:27<02:40,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0928, grad_fn=<MseLossBackward>) tensor(0.0135, grad_fn=<MulBackward0>) tensor(0.0158, grad_fn=<MulBackward0>) tensor(0.0108, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:21<01:20,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0951, grad_fn=<MseLossBackward>) tensor(0.0115, grad_fn=<MulBackward0>) tensor(0.0129, grad_fn=<MulBackward0>) tensor(0.0061, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:04<00:35,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0938, grad_fn=<MseLossBackward>) tensor(0.0072, grad_fn=<MulBackward0>) tensor(0.0112, grad_fn=<MulBackward0>) tensor(0.0100, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:44<00:00,  1.01s/it]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:52<00:00,  3.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:29<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Train scatter: [0.3167] \n",
      "         Test scatter: [0.3149]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0962, grad_fn=<MseLossBackward>) tensor(0.0102, grad_fn=<MulBackward0>) tensor(0.0156, grad_fn=<MulBackward0>) tensor(0.0103, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:50<03:42,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0912, grad_fn=<MseLossBackward>) tensor(0.0128, grad_fn=<MulBackward0>) tensor(0.0128, grad_fn=<MulBackward0>) tensor(0.0088, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:43<04:19,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0838, grad_fn=<MseLossBackward>) tensor(0.0095, grad_fn=<MulBackward0>) tensor(0.0091, grad_fn=<MulBackward0>) tensor(0.0129, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:37<03:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0889, grad_fn=<MseLossBackward>) tensor(0.0144, grad_fn=<MulBackward0>) tensor(0.0128, grad_fn=<MulBackward0>) tensor(0.0100, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:32<02:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0797, grad_fn=<MseLossBackward>) tensor(0.0083, grad_fn=<MulBackward0>) tensor(0.0118, grad_fn=<MulBackward0>) tensor(0.0099, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:22<01:07,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1043, grad_fn=<MseLossBackward>) tensor(0.0063, grad_fn=<MulBackward0>) tensor(0.0065, grad_fn=<MulBackward0>) tensor(0.0098, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:08<00:34,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1032, grad_fn=<MseLossBackward>) tensor(0.0094, grad_fn=<MulBackward0>) tensor(0.0070, grad_fn=<MulBackward0>) tensor(0.0104, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:48<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1316, grad_fn=<MseLossBackward>) tensor(0.0120, grad_fn=<MulBackward0>) tensor(0.0052, grad_fn=<MulBackward0>) tensor(0.0099, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:57<04:19,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0701, grad_fn=<MseLossBackward>) tensor(0.0075, grad_fn=<MulBackward0>) tensor(0.0085, grad_fn=<MulBackward0>) tensor(0.0125, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:51<04:21,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0550, grad_fn=<MseLossBackward>) tensor(0.0084, grad_fn=<MulBackward0>) tensor(0.0072, grad_fn=<MulBackward0>) tensor(0.0151, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:41<02:48,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0455, grad_fn=<MseLossBackward>) tensor(0.0047, grad_fn=<MulBackward0>) tensor(0.0064, grad_fn=<MulBackward0>) tensor(0.0091, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:34<02:30,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0343, grad_fn=<MseLossBackward>) tensor(0.0049, grad_fn=<MulBackward0>) tensor(0.0104, grad_fn=<MulBackward0>) tensor(0.0078, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:20<01:08,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0325, grad_fn=<MseLossBackward>) tensor(0.0060, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>) tensor(0.0069, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:05<00:43,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0322, grad_fn=<MseLossBackward>) tensor(0.0065, grad_fn=<MulBackward0>) tensor(0.0125, grad_fn=<MulBackward0>) tensor(0.0063, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:43<00:00,  1.01s/it]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:46<00:00,  3.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Train scatter: [0.1671] \n",
      "         Test scatter: [0.1659]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0397, grad_fn=<MseLossBackward>) tensor(0.0052, grad_fn=<MulBackward0>) tensor(0.0072, grad_fn=<MulBackward0>) tensor(0.0084, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:51<04:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0219, grad_fn=<MseLossBackward>) tensor(0.0043, grad_fn=<MulBackward0>) tensor(0.0118, grad_fn=<MulBackward0>) tensor(0.0089, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:39<04:12,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0296, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0071, grad_fn=<MulBackward0>) tensor(0.0075, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:29<03:12,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0186, grad_fn=<MseLossBackward>) tensor(0.0039, grad_fn=<MulBackward0>) tensor(0.0097, grad_fn=<MulBackward0>) tensor(0.0128, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:23<02:26,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0268, grad_fn=<MseLossBackward>) tensor(0.0041, grad_fn=<MulBackward0>) tensor(0.0076, grad_fn=<MulBackward0>) tensor(0.0077, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:14<01:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0347, grad_fn=<MseLossBackward>) tensor(0.0047, grad_fn=<MulBackward0>) tensor(0.0049, grad_fn=<MulBackward0>) tensor(0.0084, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:06<00:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0197, grad_fn=<MseLossBackward>) tensor(0.0055, grad_fn=<MulBackward0>) tensor(0.0071, grad_fn=<MulBackward0>) tensor(0.0055, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:40<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0411, grad_fn=<MseLossBackward>) tensor(0.0038, grad_fn=<MulBackward0>) tensor(0.0077, grad_fn=<MulBackward0>) tensor(0.0058, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:54<05:07,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0277, grad_fn=<MseLossBackward>) tensor(0.0044, grad_fn=<MulBackward0>) tensor(0.0074, grad_fn=<MulBackward0>) tensor(0.0068, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:46<04:56,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0182, grad_fn=<MseLossBackward>) tensor(0.0054, grad_fn=<MulBackward0>) tensor(0.0061, grad_fn=<MulBackward0>) tensor(0.0073, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:40<03:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0207, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0073, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:35<01:53,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0216, grad_fn=<MseLossBackward>) tensor(0.0036, grad_fn=<MulBackward0>) tensor(0.0065, grad_fn=<MulBackward0>) tensor(0.0055, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:20<01:06,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0273, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0055, grad_fn=<MulBackward0>) tensor(0.0056, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:06<00:31,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0163, grad_fn=<MseLossBackward>) tensor(0.0044, grad_fn=<MulBackward0>) tensor(0.0080, grad_fn=<MulBackward0>) tensor(0.0074, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:42<00:00,  1.00s/it]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:47<00:00,  3.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Train scatter: [0.1473] \n",
      "         Test scatter: [0.1475]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0670, grad_fn=<MseLossBackward>) tensor(0.0034, grad_fn=<MulBackward0>) tensor(0.0089, grad_fn=<MulBackward0>) tensor(0.0085, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:49<05:31,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0304, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0049, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:41<03:22,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0136, grad_fn=<MseLossBackward>) tensor(0.0036, grad_fn=<MulBackward0>) tensor(0.0058, grad_fn=<MulBackward0>) tensor(0.0067, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:28<02:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0302, grad_fn=<MseLossBackward>) tensor(0.0045, grad_fn=<MulBackward0>) tensor(0.0093, grad_fn=<MulBackward0>) tensor(0.0067, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:20<02:27,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0136, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0055, grad_fn=<MulBackward0>) tensor(0.0053, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:08<01:40,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0216, grad_fn=<MseLossBackward>) tensor(0.0038, grad_fn=<MulBackward0>) tensor(0.0069, grad_fn=<MulBackward0>) tensor(0.0075, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:59<00:39,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0190, grad_fn=<MseLossBackward>) tensor(0.0046, grad_fn=<MulBackward0>) tensor(0.0058, grad_fn=<MulBackward0>) tensor(0.0045, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:39<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0250, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0097, grad_fn=<MulBackward0>) tensor(0.0063, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:44<04:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0328, grad_fn=<MseLossBackward>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0063, grad_fn=<MulBackward0>) tensor(0.0060, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:31<04:58,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0185, grad_fn=<MseLossBackward>) tensor(0.0038, grad_fn=<MulBackward0>) tensor(0.0091, grad_fn=<MulBackward0>) tensor(0.0053, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:18<02:34,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0217, grad_fn=<MseLossBackward>) tensor(0.0030, grad_fn=<MulBackward0>) tensor(0.0058, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:11<02:50,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0126, grad_fn=<MseLossBackward>) tensor(0.0039, grad_fn=<MulBackward0>) tensor(0.0049, grad_fn=<MulBackward0>) tensor(0.0060, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:06<01:43,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0168, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0032, grad_fn=<MulBackward0>) tensor(0.0061, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:56<00:41,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0145, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0065, grad_fn=<MulBackward0>) tensor(0.0058, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:29<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:49<00:00,  3.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Train scatter: [0.1281] \n",
      "         Test scatter: [0.1291]\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0116, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0051, grad_fn=<MulBackward0>) tensor(0.0067, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:44<03:35,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0205, grad_fn=<MseLossBackward>) tensor(0.0050, grad_fn=<MulBackward0>) tensor(0.0049, grad_fn=<MulBackward0>) tensor(0.0066, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:32<03:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0115, grad_fn=<MseLossBackward>) tensor(0.0036, grad_fn=<MulBackward0>) tensor(0.0056, grad_fn=<MulBackward0>) tensor(0.0046, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:22<04:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0118, grad_fn=<MseLossBackward>) tensor(0.0043, grad_fn=<MulBackward0>) tensor(0.0046, grad_fn=<MulBackward0>) tensor(0.0063, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:10<02:32,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0056, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:01<01:21,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0171, grad_fn=<MseLossBackward>) tensor(0.0043, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0045, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:56<00:33,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0136, grad_fn=<MseLossBackward>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0065, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:34<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0324, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>) tensor(0.0039, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:47<05:22,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0110, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0030, grad_fn=<MulBackward0>) tensor(0.0050, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:37<04:26,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0224, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0043, grad_fn=<MulBackward0>) tensor(0.0042, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:29<03:20,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0108, grad_fn=<MseLossBackward>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0032, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:14<02:07,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0120, grad_fn=<MseLossBackward>) tensor(0.0030, grad_fn=<MulBackward0>) tensor(0.0034, grad_fn=<MulBackward0>) tensor(0.0042, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:01<01:20,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0122, grad_fn=<MseLossBackward>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0047, grad_fn=<MulBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:55<00:32,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0167, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0032, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:34<00:00,  1.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:49<00:00,  3.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Train scatter: [0.1185] \n",
      "         Test scatter: [0.12]\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0100, grad_fn=<MseLossBackward>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:48<05:56,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0192, grad_fn=<MseLossBackward>) tensor(0.0040, grad_fn=<MulBackward0>) tensor(0.0048, grad_fn=<MulBackward0>) tensor(0.0037, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:40<03:52,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0222, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0043, grad_fn=<MulBackward0>) tensor(0.0048, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:27<03:24,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0168, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0048, grad_fn=<MulBackward0>) tensor(0.0038, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:14<01:57,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0120, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0047, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:00<01:27,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0125, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0033, grad_fn=<MulBackward0>) tensor(0.0043, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:50<00:31,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0270, grad_fn=<MseLossBackward>) tensor(0.0032, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0043, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:27<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0906, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0038, grad_fn=<MulBackward0>) tensor(0.0054, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:51<04:36,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0165, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0030, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:43<03:03,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0174, grad_fn=<MseLossBackward>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0051, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:30<03:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0135, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0037, grad_fn=<MulBackward0>) tensor(0.0041, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:19<02:32,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0102, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0033, grad_fn=<MulBackward0>) tensor(0.0066, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:04<01:32,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0127, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0035, grad_fn=<MulBackward0>) tensor(0.0041, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:54<00:40,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0164, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:30<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:45<00:00,  3.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Train scatter: [0.1209] \n",
      "         Test scatter: [0.124]\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0133, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0037, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:54<04:44,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0129, grad_fn=<MseLossBackward>) tensor(0.0014, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0039, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:43<04:41,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0144, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0023, grad_fn=<MulBackward0>) tensor(0.0040, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:32<02:19,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0124, grad_fn=<MseLossBackward>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0033, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:19<02:23,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0222, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:02<01:10,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0143, grad_fn=<MseLossBackward>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0023, grad_fn=<MulBackward0>) tensor(0.0030, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:53<00:49,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0156, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:34<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0145, grad_fn=<MseLossBackward>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0046, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:55<04:32,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0197, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0040, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:48<03:47,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0127, grad_fn=<MseLossBackward>) tensor(0.0025, grad_fn=<MulBackward0>) tensor(0.0040, grad_fn=<MulBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:38<03:35,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0208, grad_fn=<MseLossBackward>) tensor(0.0030, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0045, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:27<02:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0120, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:16<01:16,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0180, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [05:01<00:36,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0158, grad_fn=<MseLossBackward>) tensor(0.0035, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:36<00:00,  1.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:49<00:00,  3.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Train scatter: [0.1224] \n",
      "         Test scatter: [0.1249]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0184, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:45<03:44,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0148, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0032, grad_fn=<MulBackward0>) tensor(0.0030, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:28<03:51,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0088, grad_fn=<MseLossBackward>) tensor(0.0026, grad_fn=<MulBackward0>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0056, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:12<02:50,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0119, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [02:59<02:15,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0155, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0014, grad_fn=<MulBackward0>) tensor(0.0025, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [03:52<01:32,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0213, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0039, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:44<00:43,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0097, grad_fn=<MseLossBackward>) tensor(0.0028, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:21<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0210, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:47<04:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0105, grad_fn=<MseLossBackward>) tensor(0.0023, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0035, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:35<04:45,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0119, grad_fn=<MseLossBackward>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0024, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:23<02:44,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0097, grad_fn=<MseLossBackward>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0036, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:15<03:04,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0261, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:03<01:16,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0112, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:54<00:36,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0134, grad_fn=<MseLossBackward>) tensor(0.0016, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0037, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:32<00:00,  1.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:48<00:00,  3.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Train scatter: [0.1218] \n",
      "         Test scatter: [0.1264]\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0362, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:47<04:20,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0162, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0016, grad_fn=<MulBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:32<02:35,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0119, grad_fn=<MseLossBackward>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0023, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:21<02:49,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0281, grad_fn=<MseLossBackward>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0029, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:09<02:25,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0146, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0029, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [03:56<01:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0085, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0007, grad_fn=<MulBackward0>) tensor(0.0023, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:42<00:48,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0097, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:21<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0189, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0038, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:52<03:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0237, grad_fn=<MseLossBackward>) tensor(0.0023, grad_fn=<MulBackward0>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:39<03:32,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0115, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0009, grad_fn=<MulBackward0>) tensor(0.0035, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:24<02:43,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0142, grad_fn=<MseLossBackward>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0029, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:11<02:05,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0107, grad_fn=<MseLossBackward>) tensor(0.0024, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:02<01:45,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0214, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0009, grad_fn=<MulBackward0>) tensor(0.0024, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:53<00:48,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0155, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:26<00:00,  1.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:47<00:00,  3.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Train scatter: [0.1114] \n",
      "         Test scatter: [0.114]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0987, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:50<04:06,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0172, grad_fn=<MseLossBackward>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0014, grad_fn=<MulBackward0>) tensor(0.0031, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:41<04:19,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0099, grad_fn=<MseLossBackward>) tensor(0.0029, grad_fn=<MulBackward0>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:28<02:55,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0105, grad_fn=<MseLossBackward>) tensor(0.0018, grad_fn=<MulBackward0>) tensor(0.0010, grad_fn=<MulBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:12<02:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [03:56<00:59,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0218, grad_fn=<MseLossBackward>) tensor(0.0023, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:43<00:35,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0091, grad_fn=<MseLossBackward>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:19<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0205, grad_fn=<MseLossBackward>) tensor(0.0031, grad_fn=<MulBackward0>) tensor(0.0013, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:47<04:56,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117, grad_fn=<MseLossBackward>) tensor(0.0020, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:36<04:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0153, grad_fn=<MseLossBackward>) tensor(0.0021, grad_fn=<MulBackward0>) tensor(0.0016, grad_fn=<MulBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:27<02:12,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0068, grad_fn=<MseLossBackward>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:14<02:28,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0011, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████▋                | 250/341 [04:00<01:16,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0099, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████▋       | 300/341 [04:46<00:36,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0163, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0012, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 341/341 [05:26<00:00,  1.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 341/341 [01:47<00:00,  3.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 86/86 [00:26<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Train scatter: [0.1193] \n",
      "         Test scatter: [0.1273]\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                       | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0137, grad_fn=<MseLossBackward>) tensor(0.0019, grad_fn=<MulBackward0>) tensor(0.0011, grad_fn=<MulBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████                                                     | 50/341 [00:50<04:58,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0078, grad_fn=<MseLossBackward>) tensor(0.0017, grad_fn=<MulBackward0>) tensor(0.0015, grad_fn=<MulBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|█████████████████▉                                           | 100/341 [01:35<03:39,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0350, grad_fn=<MseLossBackward>) tensor(0.0152, grad_fn=<MulBackward0>) tensor(0.0059, grad_fn=<MulBackward0>) tensor(0.0084, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████▊                                  | 150/341 [02:24<03:05,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0230, grad_fn=<MseLossBackward>) tensor(0.0088, grad_fn=<MulBackward0>) tensor(0.0027, grad_fn=<MulBackward0>) tensor(0.0099, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████████████████▊                         | 200/341 [03:10<02:25,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0206, grad_fn=<MseLossBackward>) tensor(0.0080, grad_fn=<MulBackward0>) tensor(0.0022, grad_fn=<MulBackward0>) tensor(0.0039, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████████████████████▍                 | 243/341 [03:55<01:35,  1.03it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/cj1223/.conda/envs/juptorch_julia/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2350/1764648412.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2350/15523409.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_n0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn0_latent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_node\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnode_latent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_edge\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0medge_latent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_acc, te_acc = [], []\n",
    "start=time.time()\n",
    "k=0\n",
    "for epoch in range(n_epochs):\n",
    "    print(epoch)\n",
    "    train(k)\n",
    "    if (epoch+1)%2==0:\n",
    "        train_acc, _ , _ = test(train_loader)\n",
    "        test_acc, _ , _ = test(test_loader)\n",
    "        tr_acc.append(train_acc.cpu().numpy())\n",
    "        te_acc.append(test_acc.cpu().numpy())\n",
    "        print(f'Epoch: {epoch+1:03d}, Train scatter: {np.round(train_acc.cpu().numpy(), 4)} \\n \\\n",
    "        Test scatter: {np.round(test_acc.cpu().numpy(), 4)}')\n",
    "stop=time.time()\n",
    "spent=stop-start\n",
    "print(f\"{spent:.2f} seconds spent training, {spent/n_epochs:.3f} seconds per epoch. Processed {len(data)*split*n_epochs/spent:.0f} trees per second\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86a6b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ad6a7643460>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_acc)\n",
    "plt.plot(te_acc)\n",
    "plt.plot(np.ones_like(tr_acc)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e640b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(torch.flatten(latent0[:,:2]).detach().numpy(), bins=100);\n",
    "plt.hist(torch.flatten(latent0[:,2:]).detach().numpy(), bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b891b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist(torch.flatten(latent1[:,:2]).detach().numpy(), bins=100);\n",
    "plt.hist(torch.flatten(latent1[:,2:]).detach().numpy(), bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e17eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "today = today.strftime(\"%d%m%y\")\n",
    "\n",
    "torch.save(model.state_dict(),f'trained_models/model_{epoch}_{today}.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10fd0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load=False\n",
    "if load:\n",
    "    model = GCN(hidden_channelse=128)\n",
    "    model.load_state_dict(torch.load(f'trained_models', f'model_{epoch}_{date}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6c81daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder tensor(1.0244, grad_fn=<MulBackward0>)\n",
      "encoder tensor(1.0797, grad_fn=<MulBackward0>)\n",
      "edge tensor(0.5381, grad_fn=<MulBackward0>)\n",
      "both tensor(1.1271, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('decoder', sum(p.abs().sum() for p in model.f.parameters())/sum(p.numel() for p in model.f.parameters())*100)\n",
    "\n",
    "print('encoder', sum(p.abs().sum() for p in model.g1.parameters())/sum(p.numel() for p in model.g1.parameters())*100)\n",
    "print('edge', sum(p.abs().sum() for p in model.g2.parameters())/sum(p.numel() for p in model.g2.parameters())*100)\n",
    "print('both', sum(p.abs().sum() for p in model.g3.parameters())/sum(p.numel() for p in model.g3.parameters())*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7371f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(osp.join(pointer, model_runs[k], 'trained_model', 'model.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2d621ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pysr_loader=DataLoader(test_data[:50], batch_size=batch_size, shuffle=0, num_workers=4)    \n",
    "\n",
    "y_f = []\n",
    "y_g1 = []\n",
    "y_g2 = []\n",
    "y_g3 = []\n",
    "x_g1 = []\n",
    "y_t = []\n",
    "batch = []\n",
    "i=0\n",
    "for dat in pysr_loader:\n",
    "    print(i)\n",
    "    x_g1.append(dat.x.cpu().detach().numpy())\n",
    "    yg1=model.g1(dat.x)\n",
    "    y_g1.append(yg1.cpu().detach().numpy())\n",
    "    \n",
    "    adj = dat.edge_index\n",
    "    neighbours = yg1\n",
    "    xg2 = scatter_add(neighbours[adj[0]], adj[1], dim=0)\n",
    "    x_g2.append(xg2)\n",
    "    yg2=model.g2(xg2)\n",
    "    y_g2.append(yg2.cpu().detach().numpy())\n",
    "    \n",
    "    yg1[adj[1]]+=yg2[adj[1]]\n",
    "    \n",
    "    yg3 = model.g3(yg1)\n",
    "    y_g3.append(yg3.cpu().detach().numpy())\n",
    "    x_f = global_add_pool(yg3, dat.batch)\n",
    "    \n",
    "    yf = model.f(x_f)\n",
    "    y_f.append(yf.cpu().detach().numpy())\n",
    "    \n",
    "    batch.append(dat.batch.cpu().detach().numpy())\n",
    "    y_t.append(dat.y.cpu().detach().numpy())\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e6d9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=25\n",
    "vals, counts = np.unique(batch[0], return_counts=1)\n",
    "l = np.cumsum(counts)[N-1]\n",
    "x_g1_pysr=np.vstack(x_g1[0][:l])\n",
    "y_g1_pysr=np.vstack(y_g1[0][:l])\n",
    "y_g2_pysr=np.vstack(y_g2[0][:l])\n",
    "y_g3_pysr=np.vstack(y_g3[0][:l])\n",
    "y_f_pysr=np.vstack(y_f[0][:l])\n",
    "\n",
    "b_pysr = batch[0][:l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92fb4ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on julia -O3 --threads 4 /tmp/tmpyc0xp9yp/runfile.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "\n",
      "Cycles per second: 6.440e+03\n",
      "Head worker occupation: 6.6%\n",
      "Progress: 3 / 1200 total iterations (0.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           9.573e-01  7.446e-02  (-0.39906436 + (2.5171049 * x1))\n",
      "7           7.673e-01  1.106e-01  (-0.2528663 + (x4 + (2.1287289 * x1)))\n",
      "8           7.581e-01  1.199e-02  ((2.0636811 * x1) + log_abs(0.94439185 + x4))\n",
      "9           6.789e-01  1.104e-01  (x1 + ((-0.57546425 * x0) + (3.2942095 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71885884 + (2.645293 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.491e+00  1.412e-01  ((2.6945295 * x1) + log_abs(-0.72173405 / x2))\n",
      "9           4.379e+00  2.510e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.017e+00  8.639e-02  ((2.9049654 * x1) + log_abs(sqrt_abs(log_abs(x2)) / x2))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.110e+03\n",
      "Head worker occupation: 2.9%\n",
      "Progress: 4 / 1200 total iterations (0.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765393 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25284228 + (x4 + (2.128747 * x1)))\n",
      "8           7.581e-01  1.199e-02  ((2.0636811 * x1) + log_abs(0.94439185 + x4))\n",
      "9           6.986e-01  8.170e-02  (x4 + ((-0.35670218 * x3) + (1.9270613 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71885884 + (2.645293 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.491e+00  1.412e-01  ((2.6945295 * x1) + log_abs(-0.72173405 / x2))\n",
      "9           4.379e+00  2.510e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.017e+00  8.639e-02  ((2.9049654 * x1) + log_abs(sqrt_abs(log_abs(x2)) / x2))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.930e+03\n",
      "Head worker occupation: 1.8%\n",
      "Progress: 7 / 1200 total iterations (0.583%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347527 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765505 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25280938 + (x4 + (2.128747 * x1)))\n",
      "9           7.660e-01  8.083e-04  (-0.24002542 + ((1.0874321 * x4) + (2.0947928 * x1)))\n",
      "10          7.295e-01  4.893e-02  (x1 - ((x4 * (log_abs(x4) + -1.0388708)) - x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7933848 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188488 + (2.6452794 * x1))\n",
      "7           5.172e+00  9.491e-02  (x1 + log_abs(exp(x0) / x2))\n",
      "8           4.431e+00  1.547e-01  (x0 + (-3.369158 * (x1 * log_abs(x2))))\n",
      "9           4.379e+00  1.168e-02  ((2.532024 * x1) + log_abs(log_abs(x0) / x2))\n",
      "10          4.305e+00  1.720e-02  (x0 + (2.9537868 * (x1 * log_abs(1.1906661 / x2))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.860e+03\n",
      "Head worker occupation: 1.3%\n",
      "Progress: 10 / 1200 total iterations (0.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979004 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "7           6.145e-01  2.423e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.867392 + (x1 + (-2.314182 * sqrt_abs(x2))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7936423 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188475 + (2.645304 * x1))\n",
      "6           5.024e+00  2.188e-01  ((x1 / 0.36871853) - log_abs(x1))\n",
      "7           4.676e+00  7.183e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.509e+00  3.631e-02  ((x1 / 0.37334132) - log_abs(x2 * 1.2066805))\n",
      "9           4.318e+00  4.324e-02  ((x1 / 0.34495905) - log_abs(sqrt_abs(x2) * x1))\n",
      "10          4.201e+00  2.752e-02  ((8.892884 * x1) + (-10.245522 * (x2 * sqrt_abs(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.610e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 11 / 1200 total iterations (0.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7936423 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188475 + (2.645304 * x1))\n",
      "6           5.024e+00  2.188e-01  ((x1 / 0.36871853) - log_abs(x1))\n",
      "7           4.676e+00  7.183e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.509e+00  3.631e-02  ((x1 / 0.37334132) - log_abs(x2 * 1.2066805))\n",
      "9           4.318e+00  4.324e-02  ((x1 / 0.34495905) - log_abs(sqrt_abs(x2) * x1))\n",
      "10          4.201e+00  2.752e-02  ((8.892884 * x1) + (-10.245522 * (x2 * sqrt_abs(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 3.380e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 13 / 1200 total iterations (1.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.71897423 + (2.6452804 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 - log_abs(x2)) + x1)\n",
      "7           4.676e+00  4.954e-02  ((x1 / sqrt_abs(-0.11523554)) - log_abs(x2))\n",
      "8           4.560e+00  2.520e-02  ((x1 - (log_abs(x2) - x0)) + x1)\n",
      "10          4.217e+00  3.903e-02  (-0.93081266 + ((-1.5788529 * log_abs(x2)) + (2.7227342 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.080e+03\n",
      "Head worker occupation: 0.8%\n",
      "Progress: 14 / 1200 total iterations (1.167%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797928 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.178e-01  9.897e-04  (x1 - (0.009313339 + log10_abs(x2)))\n",
      "7           6.145e-01  5.290e-03  (x1 - (log_abs(sqrt_abs(x2)) - -0.10132865))\n",
      "8           5.698e-01  7.561e-02  (1.8673731 + (x1 + (-2.3141558 * sqrt_abs(x2))))\n",
      "10          5.679e-01  1.624e-03  (x1 + (-1.0557032 * log10_abs(x2 + (-0.1134993 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7940874 * x1)\n",
      "5           6.253e+00  3.838e-02  (0.7188683 + (2.6452758 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 - log_abs(x2)) + x1)\n",
      "8           4.491e+00  4.498e-02  ((2.6943352 * x1) + log_abs(0.7219004 / x2))\n",
      "10          3.967e+00  6.198e-02  ((3.6765597 * x1) + log_abs(-0.8480846 + (0.8631841 / x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.330e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 17 / 1200 total iterations (1.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3798684 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.95245224 * log10_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.965e-01  3.125e-02  (x1 + (-0.93270797 * log10_abs(0.06882628 + x2)))\n",
      "9           5.494e-01  8.224e-02  (x1 - log10_abs((x3 * log10_abs(x0)) + x2))\n",
      "10          5.306e-01  3.486e-02  (x1 - log10_abs((x3 * log10_abs(sqrt_abs(x0))) + x2))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71884656 + (2.6452959 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "7           4.044e+00  1.946e-01  ((-9.898346 * x2) + (8.19939 * x1))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.100e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 18 / 1200 total iterations (1.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3798684 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.95245224 * log10_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.965e-01  3.125e-02  (x1 + (-0.93270797 * log10_abs(0.06882628 + x2)))\n",
      "9           5.494e-01  8.224e-02  (x1 - log10_abs((x3 * log10_abs(x0)) + x2))\n",
      "10          5.306e-01  3.486e-02  (x1 - log10_abs((x3 * log10_abs(sqrt_abs(x0))) + x2))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.088e+00  4.701e-01  (4.234271 * x2)\n",
      "5           8.302e-01  1.354e-01  (x4 - (x1 / -0.49075502))\n",
      "7           7.658e-01  4.035e-02  (x1 + (x4 + (1.9295096 * x2)))\n",
      "10          7.068e-01  2.676e-02  (x1 + (x4 + (x2 * exp(sqrt_abs(log_abs(x2))))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.150e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 20 / 1200 total iterations (1.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37978694 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41363257 * log_abs(x2)))\n",
      "7           6.155e-01  4.403e-03  (x1 - log10_abs(log10_abs(-1.008061) + x2))\n",
      "8           5.981e-01  2.865e-02  (x1 + (-0.92408365 * log10_abs(0.05345475 + x2)))\n",
      "9           5.918e-01  1.052e-02  (x1 - (log10_abs(x2) * sqrt_abs(x0 + 1.1087832)))\n",
      "10          5.659e-01  4.476e-02  (x1 + (-1.07813 * log10_abs(x2 + (-0.09936239 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.076545 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25267908 + (x4 + (2.1288564 * x1)))\n",
      "9           6.693e-01  6.832e-02  (-0.5448855 + ((-0.5488192 * x3) + (2.3172386 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 2.680e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 21 / 1200 total iterations (1.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.990e-01  8.240e-02  (x1 + 0.34660563)\n",
      "4           6.190e-01  2.553e-01  (x1 - log10_abs(x2))\n",
      "7           5.908e-01  1.554e-02  ((2.9686167 * x1) + (-3.5632546 * x2))\n",
      "8           5.679e-01  3.962e-02  ((2.3849034 - log_abs(x1)) * (x1 - x2))\n",
      "10          5.659e-01  1.701e-03  (x1 + (-1.07813 * log10_abs(x2 + (-0.09936239 * x3))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.434756 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.076545 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.25267908 + (x4 + (2.1288564 * x1)))\n",
      "9           6.693e-01  6.832e-02  (-0.5448855 + ((-0.5488192 * x3) + (2.3172386 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.990e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 24 / 1200 total iterations (2.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979323 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.143e-01  3.843e-03  (x1 - log10_abs(x2 * 1.2160699))\n",
      "8           5.966e-01  1.461e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "10          5.663e-01  2.607e-02  (x1 - log10_abs((x2 + (x3 * -0.09657265)) * -1.0948982))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.0765564 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.2530826 + (x4 + (2.1288114 * x1)))\n",
      "9           6.986e-01  4.684e-02  (x4 + ((-0.35670036 * x3) + (1.9270647 * x1)))\n",
      "10          6.585e-01  5.921e-02  (x4 + ((3.4497137 * x2) + (x3 * log10_abs(x0))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.770e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 25 / 1200 total iterations (2.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37979323 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.143e-01  3.843e-03  (x1 - log10_abs(x2 * 1.2160699))\n",
      "8           5.966e-01  1.461e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "10          5.663e-01  2.607e-02  (x1 - log10_abs((x2 + (x3 * -0.09657265)) * -1.0948982))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.131e+00  4.509e-01  (2.5918505 * x1)\n",
      "4           1.117e+00  1.246e-02  (sqrt_abs(5.5143814) * x1)\n",
      "5           8.352e-01  2.909e-01  ((x1 * 1.9893233) + x4)\n",
      "7           7.673e-01  4.239e-02  (-0.25282595 + (x4 + (2.128735 * x1)))\n",
      "8           6.761e-01  1.265e-01  (x4 + (x1 * sqrt_abs(4.6921225 / x1)))\n",
      "10          6.077e-01  5.331e-02  (x2 + (x4 + (x1 * log_abs(5.268682 / x1))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.750e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 27 / 1200 total iterations (2.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797861 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41367182 * log_abs(x2)))\n",
      "8           5.966e-01  1.780e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.265e+00  3.950e-01  (x1 + x1)\n",
      "4           1.111e+00  1.295e-01  (x1 * sqrt_abs(6.014003))\n",
      "5           8.822e-01  2.308e-01  ((x1 * 3.2053354) - x0)\n",
      "6           8.366e-01  5.304e-02  (x1 * sqrt_abs(-6.5326552 / x1))\n",
      "7           7.727e-01  7.943e-02  ((-0.6377825 * x0) + (2.913713 * x1))\n",
      "8           6.761e-01  1.336e-01  (x4 + (x1 * sqrt_abs(4.6921225 / x1)))\n",
      "9           6.611e-01  2.232e-02  (-0.3417246 + ((-0.59988934 * x0) + (2.9557881 * x1)))\n",
      "10          6.077e-01  8.430e-02  (x2 + (x4 + (x1 * log_abs(5.268682 / x1))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 2.620e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 28 / 1200 total iterations (2.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797861 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.182e-01  6.523e-04  (x1 + (-0.41367182 * log_abs(x2)))\n",
      "8           5.966e-01  1.780e-02  (x1 + (-0.94355124 * log10_abs(0.06880875 + x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.232e+00  4.080e-01  (x1 / 0.48811185)\n",
      "5           8.337e-01  1.954e-01  ((x1 + x4) + x1)\n",
      "8           8.279e-01  2.349e-03  (x1 - ((x3 - x1) + sqrt_abs(x2)))\n",
      "9           6.693e-01  2.127e-01  (-0.5449041 + ((-0.5488431 * x3) + (2.317193 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           7.848e+00  1.477e-01  (x1 + x0)\n",
      "4           6.749e+00  1.510e-01  (x1 - log_abs(x2))\n",
      "6           5.172e+00  1.330e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  (((x4 + x0) + x1) - log_abs(x2))\n",
      "10          4.191e+00  3.910e-02  ((x1 + (x0 + x4)) - log_abs(x1 - 0.3692989))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 2.770e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 30 / 1200 total iterations (2.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972298 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85042274 / x2))\n",
      "8           6.078e-01  5.104e-03  (x1 + log10_abs(-0.3060487 + (0.8593486 / x2)))\n",
      "9           5.971e-01  1.785e-02  ((x1 - log10_abs(x2 * sqrt_abs(x1))) - 0.18158987)\n",
      "10          5.677e-01  5.047e-02  (x1 + log10_abs((-1.0497826 / x2) + (-0.7018149 * x3)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.232e+00  4.080e-01  (x1 / 0.48811185)\n",
      "5           8.337e-01  1.954e-01  ((x1 + x4) + x1)\n",
      "8           8.279e-01  2.349e-03  (x1 - ((x3 - x1) + sqrt_abs(x2)))\n",
      "9           6.693e-01  2.127e-01  (-0.5449041 + ((-0.5488431 * x3) + (2.317193 * x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "3           7.265e+00  1.273e-01  (x1 + x1)\n",
      "4           6.749e+00  7.369e-02  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71886116 + (2.6453934 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "8           4.560e+00  3.737e-02  (((x0 + x1) + x1) - log_abs(x2))\n",
      "10          4.217e+00  3.903e-02  (-0.9306045 + ((-1.5786276 * log_abs(x2)) + (2.7231658 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.210e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 33 / 1200 total iterations (2.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972295 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85011387 / x2))\n",
      "7           5.908e-01  3.858e-02  ((-3.5634954 * x2) + (2.96877 * x1))\n",
      "9           5.882e-01  2.222e-03  (-0.073791474 + ((3.1524632 * x1) + (-3.8719606 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347527 * x1)\n",
      "5           8.290e-01  1.464e-01  (x4 + (2.07655 * x1))\n",
      "7           7.673e-01  3.867e-02  (-0.2528399 + (x4 + (2.128747 * x1)))\n",
      "9           7.532e-01  9.242e-03  (-0.1331652 + (x2 + (x4 + (1.5579417 * x1))))\n",
      "10          6.607e-01  1.311e-01  (x4 + ((1.8001357 * x1) + log10_abs(0.34272045 + x1)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "3           7.265e+00  1.273e-01  (x1 + x1)\n",
      "4           6.749e+00  7.369e-02  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.71886116 + (2.6453934 * x1))\n",
      "6           4.913e+00  2.411e-01  ((x1 + x1) - log_abs(x2))\n",
      "8           4.560e+00  3.737e-02  (((x0 + x1) + x1) - log_abs(x2))\n",
      "10          4.217e+00  3.903e-02  (-0.9306045 + ((-1.5786276 * log_abs(x2)) + (2.7231658 * x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.070e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 36 / 1200 total iterations (3.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37972295 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(0.85011387 / x2))\n",
      "7           5.908e-01  3.858e-02  ((-3.5634954 * x2) + (2.96877 * x1))\n",
      "9           5.882e-01  2.222e-03  (-0.073791474 + ((3.1524632 * x1) + (-3.8719606 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.793656 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188273 + (2.6452773 * x1))\n",
      "6           5.172e+00  1.898e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  ((x1 + (x0 + x4)) - log_abs(x2))\n",
      "10          4.112e+00  4.857e-02  (((x1 + x1) - log_abs(x1 - 0.2593729)) / 0.6445904)\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.480e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 39 / 1200 total iterations (3.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.3797341 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "6           6.140e-01  4.015e-03  (x1 + log10_abs(-0.8504216 / x2))\n",
      "7           6.119e-01  3.561e-03  (x1 + sqrt_abs(log10_abs(-0.36682373 + x1)))\n",
      "9           5.966e-01  1.267e-02  (x1 + log10_abs(log10_abs(0.15950276 * x3) / x2))\n",
      "10          5.824e-01  2.393e-02  (x1 + log10_abs(-0.90379727 / (x2 + (0.115976155 * x0))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.793656 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188273 + (2.6452773 * x1))\n",
      "6           5.172e+00  1.898e-01  ((x1 + x0) - log_abs(x2))\n",
      "8           4.532e+00  6.606e-02  ((x1 + (x0 + x4)) - log_abs(x2))\n",
      "10          4.112e+00  4.857e-02  (((x1 + x1) - log_abs(x1 - 0.2593729)) / 0.6445904)\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.480e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 42 / 1200 total iterations (3.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.421e-01  4.605e-01  x1\n",
      "3           7.979e-01  8.308e-02  (0.37967104 + x1)\n",
      "4           6.190e-01  2.539e-01  (x1 - log10_abs(x2))\n",
      "8           5.698e-01  2.072e-02  (1.8674098 + (x1 + (-2.314192 * sqrt_abs(x2))))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.787e+00  7.562e-01  x1\n",
      "3           1.111e+00  4.598e-01  (2.4347353 * x1)\n",
      "5           9.994e-01  5.292e-02  ((x1 + x4) + x2)\n",
      "6           8.367e-01  1.777e-01  (sqrt_abs(-6.602573 / x1) * x1)\n",
      "8           7.983e-01  2.350e-02  ((x1 * 2.190498) - (sqrt_abs(x2) + x3))\n",
      "9           6.693e-01  1.763e-01  (-0.5448973 + ((-0.5488515 * x3) + (2.3172061 * x1)))\n",
      "10          6.321e-01  5.710e-02  ((x1 * 2.3593307) - (sqrt_abs(x2) + (x3 * 0.59716284)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           9.371e+00  2.264e-01  x1\n",
      "2           9.097e+00  2.968e-02  exp(x1)\n",
      "3           6.752e+00  2.981e-01  (2.7937005 * x1)\n",
      "4           6.749e+00  4.822e-04  (x1 - log_abs(x2))\n",
      "5           6.253e+00  7.627e-02  (0.7188429 + (2.6452808 * x1))\n",
      "6           4.913e+00  2.411e-01  (x1 + (x1 - log_abs(x2)))\n",
      "8           4.679e+00  2.442e-02  (x1 + (x1 - log_abs(x1 - 0.45799425)))\n",
      "10          3.656e+00  1.234e-01  (x0 + (x1 + (-1.9220711 * log_abs(-0.39283225 + x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Killing process... will return when done.\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "g1_equations = pysr(\n",
    "    X=x_g1_pysr, y=y_g1_pysr,\n",
    "    procs=4,\n",
    "    niterations=20,\n",
    "    populations=20,\n",
    "    useFrequency=True,\n",
    "    multithreading=True, \n",
    "    binary_operators=[\"plus\", \"sub\", \"mult\", \"div\"],\n",
    "    unary_operators = ['log10_abs', 'sqrt_abs', 'exp', 'log'], ##still need a general power law\n",
    "    batching=1, \n",
    "    batchSize=256,\n",
    "    maxsize=10, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ef6a182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                                4\n",
       "MSE                                                0.618991\n",
       "score                                              0.253895\n",
       "Equation                               (x1 - log10_abs(x2))\n",
       "sympy_format                      x1 - log(Abs(x2))/log(10)\n",
       "lambda_format    PySRFunction(X=>x1 - log(Abs(x2))/log(10))\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[0].sort_values(by='score', ascending=False).iloc[0]\n",
    "a=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3ce5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                          3\n",
       "MSE                                          1.143048\n",
       "score                                        0.445626\n",
       "Equation                            (x1 / 0.37977126)\n",
       "sympy_format                      2.63316397349289*x1\n",
       "lambda_format    PySRFunction(X=>2.63316397349289*x1)\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[1].sort_values(by='score', ascending=False).iloc[0]\n",
    "b=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "33090e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Complexity                                   3\n",
       "MSE                                   6.751929\n",
       "score                                 0.298143\n",
       "Equation                      (2.7937005 * x1)\n",
       "sympy_format                      2.7937005*x1\n",
       "lambda_format    PySRFunction(X=>2.7937005*x1)\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = g1_equations[2].sort_values(by='score', ascending=False).iloc[0]\n",
    "c=eq['lambda_format'](x_g1[0])\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5414aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_an_pysr = np.vstack([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838585ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on julia -O3 --threads 4 /tmp/tmpgyms7yzv/runfile.jl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/.conda/envs/juptorch_julia/lib/python3.9/site-packages/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "\n",
      "Cycles per second: 5.180e+03\n",
      "Head worker occupation: 8.3%\n",
      "Progress: 2 / 1200 total iterations (0.167%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2790511\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.2852532 + (-0.26651594 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.950e+03\n",
      "Head worker occupation: 5.1%\n",
      "Progress: 3 / 1200 total iterations (0.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.440e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 4 / 1200 total iterations (0.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.710e+03\n",
      "Head worker occupation: 1.1%\n",
      "Progress: 5 / 1200 total iterations (0.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           8.282e-01  6.981e-02  sqrt_abs(1.3321475 - x0)\n",
      "5           5.994e-01  3.233e-01  (1.285261 + (-0.26652515 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.430e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 6 / 1200 total iterations (0.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.285257 + (-0.2665256 * x1))\n",
      "6           5.982e-01  2.089e-03  sqrt_abs(1.9554164 + (-0.6193561 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.740e+03\n",
      "Head worker occupation: 0.9%\n",
      "Progress: 8 / 1200 total iterations (0.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.750e+03\n",
      "Head worker occupation: 0.8%\n",
      "Progress: 9 / 1200 total iterations (0.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 3.420e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 10 / 1200 total iterations (0.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.350e+03\n",
      "Head worker occupation: 0.7%\n",
      "Progress: 13 / 1200 total iterations (1.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.095e-03  sqrt_abs(-1.9597083 + (0.6207334 * x1))\n",
      "8           5.827e-01  1.316e-02  sqrt_abs(log_abs(-0.73548853 + (-5.0798693 / exp(x1))))\n",
      "10          5.551e-01  2.423e-02  sqrt_abs(log_abs(-0.76794374 + ((-4.9026785 + x2) / exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.450e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 15 / 1200 total iterations (1.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852541 + (-0.26650703 * x1))\n",
      "6           5.982e-01  2.093e-03  sqrt_abs(1.9594429 + (-0.6206558 * x1))\n",
      "8           5.891e-01  7.699e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.648e-01  2.098e-02  sqrt_abs(0.11480782 + exp(1.308776 + (-0.45345652 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.080e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 16 / 1200 total iterations (1.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2788731\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.648e-01  2.098e-02  sqrt_abs(0.11480782 + exp(1.308776 + (-0.45345652 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.810e+03\n",
      "Head worker occupation: 0.6%\n",
      "Progress: 19 / 1200 total iterations (1.583%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2786434\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171164 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.110e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 21 / 1200 total iterations (1.750%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 5.100e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 22 / 1200 total iterations (1.833%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.060e+03\n",
      "Head worker occupation: 0.5%\n",
      "Progress: 23 / 1200 total iterations (1.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25171337 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852596 + (-0.26653382 * x1))\n",
      "6           5.982e-01  2.096e-03  sqrt_abs(1.959238 + (-0.6205837 * x1))\n",
      "8           5.891e-01  7.697e-03  sqrt_abs(exp(1.3203932 + (-0.38085437 * exp(x1))))\n",
      "10          5.699e-01  1.651e-02  sqrt_abs(sqrt_abs(exp(log10_abs(exp(x2) + -1.2699978) - x1)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.730e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 25 / 1200 total iterations (2.083%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25174072 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852515 + (-0.2665252 * x1))\n",
      "6           5.982e-01  2.083e-03  sqrt_abs(1.9546335 + (-0.6191466 * x1))\n",
      "7           5.891e-01  1.541e-02  exp(0.6601062 + (-0.19049618 * exp(x1)))\n",
      "9           5.862e-01  2.450e-03  exp((x1 + (sqrt_abs(x1) - 1.815573)) * -0.24319673)\n",
      "10          5.570e-01  5.109e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.650e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 28 / 1200 total iterations (2.333%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.25174072 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852579 + (-0.26652578 * x1))\n",
      "6           5.982e-01  2.072e-03  sqrt_abs(1.9627051 + (-0.62186176 * x1))\n",
      "7           5.873e-01  1.847e-02  (sqrt_abs(sqrt_abs(9.484525) - x1) + -0.39439014)\n",
      "8           5.845e-01  4.689e-03  (-0.30447757 + sqrt_abs(2.8036215 + (-0.8870912 * x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 5.130e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 30 / 1200 total iterations (2.500%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2789322\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852509 + (-0.26652473 * x1))\n",
      "6           5.982e-01  2.082e-03  sqrt_abs(1.9564418 + (-0.61975557 * x1))\n",
      "7           5.891e-01  1.541e-02  exp(0.6601307 + (-0.19048771 * exp(x1)))\n",
      "8           5.845e-01  7.735e-03  (-0.30447757 + sqrt_abs(2.8036215 + (-0.8870912 * x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.150e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 32 / 1200 total iterations (2.667%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852594 + (-0.26652542 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042943 + ((0.07095845 * x2) + (-0.33296615 * x1)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cycles per second: 4.950e+03\n",
      "Head worker occupation: 0.4%\n",
      "Progress: 35 / 1200 total iterations (2.917%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852598 + (-0.26652578 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.910e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 36 / 1200 total iterations (3.000%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852598 + (-0.26652578 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 4.030e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 39 / 1200 total iterations (3.250%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  1.192e-07  1.278901\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517181 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852643 + (-0.26654008 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "8           5.845e-01  1.417e-03  (-0.30342862 + (0.94143367 * sqrt_abs(-3.159909 + x1)))\n",
      "9           5.665e-01  3.136e-02  (1.2042917 + ((-0.3329634 * x1) + (0.07096216 * x2)))\n",
      "10          5.570e-01  1.690e-02  ((-0.34644172 * x1) + (1.144835 * exp(0.072146975 * x2)))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Cycles per second: 3.870e+03\n",
      "Head worker occupation: 0.3%\n",
      "Progress: 41 / 1200 total iterations (3.417%)\n",
      "==============================\n",
      "Best equations for output 1\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.406e-02  -0.000e+00  -0.3479579\n",
      "5           1.762e-02  7.788e-02  (-0.3471601 + (-0.032939747 * x1))\n",
      "8           1.757e-02  9.525e-04  (-0.7191795 + exp(-1.0134043 + (-0.092179745 * x1)))\n",
      "9           1.641e-02  6.816e-02  (-0.362665 + ((-0.045649357 * x1) + (0.013580261 * x2)))\n",
      "\n",
      "==============================\n",
      "Best equations for output 2\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.068e-02  -0.000e+00  -0.23073237\n",
      "5           1.325e-02  1.114e-01  (-0.23157844 + (0.035398062 * x1))\n",
      "\n",
      "==============================\n",
      "Best equations for output 3\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           1.021e+00  -0.000e+00  1.2790481\n",
      "4           6.409e-01  1.553e-01  exp(-0.2517328 * x1)\n",
      "5           5.994e-01  6.693e-02  (1.2852972 + (-0.26651946 * x1))\n",
      "6           5.862e-01  2.239e-02  (-0.4006784 + sqrt_abs(-3.1526465 + x1))\n",
      "7           5.752e-01  1.888e-02  log10_abs(exp(1.9263862 - x1) + x2)\n",
      "8           5.707e-01  7.786e-03  log10_abs(exp(exp(0.74600375) - x1) + x2)\n",
      "9           5.671e-01  6.467e-03  log10_abs(x2 + exp(2.390812 + (-0.8634285 * x1)))\n",
      "10          5.169e-01  9.272e-02  log10_abs(x2 + exp(4.511248 + (-1.0051906 * exp(x1))))\n",
      "\n",
      "==============================\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    }
   ],
   "source": [
    "g2_equations = pysr(\n",
    "    X=y_g1_pysr, y=y_g2_pysr,\n",
    "    procs=4,\n",
    "    niterations=20,\n",
    "    populations=20,\n",
    "    useFrequency=True,\n",
    "    multithreading=True, \n",
    "    binary_operators=[\"plus\", \"sub\", \"mult\", \"div\", 'pow'],\n",
    "    unary_operators = ['log10_abs', 'sqrt_abs', 'exp', 'log'], ##still need a general power law\n",
    "    batching=1, \n",
    "    batchSize=256,\n",
    "    maxsize=10, update=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aca42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
