{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f675a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, pickle, os, sys, argparse\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os.path as osp\n",
    "\n",
    "def convert(d,p):\n",
    "    dfin=[]\n",
    "    if len(p)!=len(np.unique(p)):\n",
    "        print('Wrong order of prog/desc')\n",
    "    else:\n",
    "        no=d[0]\n",
    "        for desc in d:\n",
    "            if desc==no:\n",
    "                dfin.append(0)\n",
    "            else:\n",
    "                dfin.append(p.index(desc)+1)\n",
    "    return dfin, np.arange(1, 1+len(p))\n",
    "\n",
    "def create_graphs(tcols=[0,2,4,5,6,7,8,10,28], target=8, lim=10.5, save=True, case='test', transform='quantile', maxs=[1,1,1]):\n",
    "    dat=[]\n",
    "    tcols=np.array(tcols)\n",
    "    raw_path='~/../../../tigress/mcranmer/merger_trees/isotrees/'\n",
    "    target_path='~/../../../tigress/mcranmer/merger_trees/samout/'\n",
    "    transform_path=f\"../../../../../scratch/gpfs/cj1223/GraphStorage/transformers/{transform}_allfeat.pkl\"\n",
    "    scaler=pickle.load(open(transform_path, 'rb'))\n",
    "    not_include=np.array([0,1,0,1,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\\\n",
    "        0,0,0,0,0,0,0,0,0,0,0]) \n",
    "    is_cat=[0,1,0,1,1,1,1,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,\\\n",
    "        0,0,0,0,0,0,0,0,0,0,0]\n",
    "    is_cat=np.array([bool(i) for i in is_cat])\n",
    "    icols=np.array([1,3])\n",
    "    load_cols=np.sort(np.concatenate([tcols,icols]))\n",
    "    not_include=not_include[load_cols]\n",
    "    \n",
    "    for i in range(0, maxs[0]):\n",
    "        for j in range(0,maxs[1]):\n",
    "            for k in range(0, maxs[2]):\n",
    "                start=time.time()\n",
    "                print(f'Loading isotree {i}_{j}_{k}')\n",
    "                pd1=pd.read_table(raw_path+f'isotree_{i}_{j}_{k}.dat', skiprows=0, delimiter='\\s+', usecols=load_cols)\n",
    "                print(f'isotree {i}_{j}_{k} loaded, restructuring')\n",
    "                \n",
    "                raw=pd1.drop(axis=0, index=np.arange(50)).reset_index()\n",
    "                del pd1\n",
    "                \n",
    "                trees=raw[raw.isna()['desc_id(3)']] \n",
    "                halos=raw[~raw.isna()['desc_id(3)']]\n",
    "                del raw\n",
    "                \n",
    "                ##########################################\n",
    "                ### End loading, begin dtype changing  ###\n",
    "                ##########################################\n",
    "                \n",
    "                mask=halos.dtypes.values[1:]=='O'\n",
    "\n",
    "                f,it=\"float64\", \"int64\"\n",
    "                map1=[]\n",
    "                map2=[]\n",
    "                q=0\n",
    "                for l in load_cols:\n",
    "                    q+=1\n",
    "                    if l<25:\n",
    "                        map1.append(l)\n",
    "                        map2.append(q)\n",
    "                map1=np.array(map1)\n",
    "                map2=np.array(map2)\n",
    "\n",
    "                castto=np.array([f,it,f,it,it,it,it,it,it,f,f,f,f,f,it,f,f,f,f,f,f,f,f,f,f])\n",
    "\n",
    "                castto=castto[map1]\n",
    "                dicts = {}\n",
    "                keys = halos.columns[map2]\n",
    "                for d, key in enumerate(keys):\n",
    "                        dicts[key] = castto[d]\n",
    "\n",
    "                halos=halos.astype(dicts)\n",
    "                \n",
    "                ###################################################\n",
    "                ### end dtype, begin inital hardcoded scaling   ###\n",
    "                ###################################################\n",
    "                \n",
    "                ## initial scaling\n",
    "                hard=[0,2,15]\n",
    "                def logit(x):\n",
    "                    return np.log10((x+0.001)/(1.01-x))\n",
    "                ##logit transform the hards\n",
    "                for h in hard:\n",
    "                    if h in tcols:\n",
    "                        h1=np.where(load_cols==h)[0][0]+1\n",
    "                        halos[halos.columns[h1]]=logit(halos[halos.columns[h1]])\n",
    "\n",
    "                log=[10,38, 39, 40, 41, 42]\n",
    "                #simple log for the rest\n",
    "                def logt(x):\n",
    "                    return np.log10(x+1)\n",
    "                for l in log:\n",
    "                    if l in tcols:\n",
    "                        l1=np.where(load_cols==l)[0][0]+1\n",
    "                        halos[halos.columns[l1]]=logt(halos[halos.columns[l1]])\n",
    "\n",
    "                scale_cols=np.array(tcols[~is_cat[tcols]])\n",
    "\n",
    "                print(f'Splitting to tree and scaling')\n",
    "                spli=np.split(np.array(halos)[:,1:], np.array(trees.iloc[1:].index)-np.arange(1,len(trees.index)))\n",
    "                del halos\n",
    "                splits=[]\n",
    "                for s in spli:\n",
    "                    if s[0,np.where(load_cols==10)[0][0]]>lim:\n",
    "                        splits.append(s)\n",
    "                splits=np.array(splits, dtype=object)\n",
    "                split=[]\n",
    "                for tree in splits:\n",
    "                    s=tree[np.logical_or(tree[:,3] == -1,tree[:,4]!=1)]\n",
    "                    for n in scale_cols:\n",
    "                        n1=np.where(load_cols==n)[0][0]\n",
    "                        s[:,n1]=scaler[n].transform(s[:,n1].reshape(-1,1)).reshape(np.shape(s[:,n1]))\n",
    "                    split.append(s)\n",
    "                split=np.array(split, dtype=object)\n",
    "                print('Split done')\n",
    "                print('Loading targets')\n",
    "                ex=f'{i}_{j}_{k}/galprop_0-99.dat'\n",
    "                pdc=pd.read_table(target_path+ex, skiprows=0, delimiter=',', nrows=41, header=None)\n",
    "                newcols=pdc.iloc[:,0]\n",
    "                pds=pd.read_table(target_path+ex, skiprows=41, delimiter='\\s+', header=None)\n",
    "                pds.columns=np.array(newcols)\n",
    "                pd0=pds[pds[pds.columns[3]]==0.00] # subhaloes\n",
    "                pdcen=pd0[(pd0[pds.columns[1]]==pd0[pds.columns[2]])] ##central haloes\n",
    "\n",
    "                rhalid=np.array(pdcen[pds.columns[2]])\n",
    "                del pd0\n",
    "                del pds\n",
    "                halwgal=[]\n",
    "                ids=[]\n",
    "                out=[]\n",
    "                for idx, tree in enumerate(split): #split up into tree\n",
    "                    if tree[0,1] in rhalid:\n",
    "                        if len(tree)>20000:\n",
    "                            print(len(tree))\n",
    "                        else:\n",
    "                            halwgal.append(tree)\n",
    "                            ids.append(idx)\n",
    "                            index=np.where(rhalid==tree[0,1])\n",
    "                            out.append(np.array(pdcen.iloc[index])[0][target]) ## target variable\n",
    "                del pdcen\n",
    "\n",
    "                hraw=np.array(splits[ids], dtype=object)\n",
    "                hals=[]\n",
    "                pr,de=[],[]\n",
    "                discards=[]\n",
    "                print('Making merger tree')\n",
    "                for n in tqdm(range(len(halwgal))):\n",
    "                    h=halwgal[n]\n",
    "                    roots=h[h[:,4]==0]\n",
    "                    mergers=h[h[:,4]>1]\n",
    "                    final=h[h[:,3]==-1]\n",
    "                    pro, des=[],[]\n",
    "                    discarded=[]\n",
    "                    for mid in mergers[:,1]:\n",
    "                        k=1\n",
    "                        descid=hraw[n][:,3][np.where(mid==hraw[n][:,1])] ##descendant ID of raw where the id of the merger is\n",
    "                        while descid not in mergers[:,1] and descid!=-1: \n",
    "                            k+=1\n",
    "                            descid=hraw[n][:,3][np.where(descid==hraw[n][:,1])]\n",
    "                        pro.append(mid)\n",
    "                        if descid!=-1:\n",
    "                            des.append(descid[0])\n",
    "                        else:\n",
    "                            des.append(hraw[n][:,3][np.where(mid==hraw[n][:,1])][0])\n",
    "                        discarded.append(1/k) \n",
    "\n",
    "                    for r in roots:\n",
    "                        descid=hraw[n][:,3][np.where(r[1]==hraw[n][:,1])] \n",
    "                        k=1\n",
    "                        while descid not in mergers[:,1] and descid!=[-1]: ##could add 1/k\n",
    "                            k+=1\n",
    "                            descid=hraw[n][:,3][np.where(descid==hraw[n][:,1])]#consider adding the number of steps it went through\n",
    "                        pro.append(r[1])\n",
    "                        if descid!=-1:\n",
    "                            des.append(descid[0])\n",
    "                        else:\n",
    "                            des.append(hraw[n][:,3][np.where(r[1]==hraw[n][:,1])][0])\n",
    "                        discarded.append(1/k) \n",
    "\n",
    "                    discards.append(np.array(discarded))\n",
    "                    des,pro=convert(des, pro)\n",
    "                    hal2=np.vstack([final,mergers,roots])\n",
    "                    global mass_index\n",
    "                    if n==0:\n",
    "                        fcols=[i for i in range(len(hal2[0])) if not_include[i]==0] ##choose non-id cols to carry forward\n",
    "                        mask=np.array([bool(i) for i in not_include])\n",
    "                        idcols=load_cols[~mask] ##choose non-id cols to carry forward\n",
    "                        mass_id=np.where(np.array(idcols)==10)[0][0]\n",
    "                    hal2=hal2[:,fcols] ##take away id's\n",
    "                    hals.append(hal2)\n",
    "                    pr.append([int(p) for p in pro])\n",
    "                    de.append([int(d) for d in des])\n",
    "                hals=np.array(hals,dtype=object)\n",
    "                out=np.log10(out) #homemade scaling\n",
    "                for n in tqdm(range(len(out))):\n",
    "                    edge_index = torch.tensor([pr[n],de[n]], dtype=torch.long)\n",
    "                    x = torch.tensor(hals[n], dtype=torch.float)\n",
    "                    y=torch.tensor(out[n], dtype=torch.float)\n",
    "                    edge_attr=torch.tensor(discards[n], dtype=torch.float)\n",
    "                    graph=Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "                    dat.append(graph)\n",
    "                stop=time.time()\n",
    "                print(f'Done with tree. \\n Time elapsed {stop-start} s')\n",
    "    if save:\n",
    "        print(\"Saving dataset\")\n",
    "        if not osp.exists(f'../../../../../scratch/gpfs/cj1223/GraphStorage/{case}'):\n",
    "            os.mkdir(f'../../../../../scratch/gpfs/cj1223/GraphStorage/{case}')\n",
    "\n",
    "        with open(f'../../../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl', 'wb') as handle:\n",
    "            pickle.dump(dat, handle)\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50494de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading isotree 0_0_0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cj1223/../../../tigress/mcranmer/merger_trees/isotrees/isotree_0_0_0.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6270/2911467718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmaxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6270/1791952983.py\u001b[0m in \u001b[0;36mcreate_graphs\u001b[0;34m(tcols, target, lim, save, case, transform, maxs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loading isotree {i}_{j}_{k}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mpd1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf'isotree_{i}_{j}_{k}.dat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'isotree {i}_{j}_{k} loaded, restructuring'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, encoding_errors, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cj1223/../../../tigress/mcranmer/merger_trees/isotrees/isotree_0_0_0.dat'"
     ]
    }
   ],
   "source": [
    "all_cols=np.array([0,2,4,5,6,7,8,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,35]+list(range(37,60)))\n",
    "transform='quantile'\n",
    "case='test'\n",
    "maxs=[1,1,1]\n",
    "dat=create_graphs(tcols=all_cols, maxs=maxs, lim=10, save=1, case=case, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4182560",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
